Nom du fichier: mikheev J02-3002.pdf

Titre: Periods, Capitalized Words, etc. 

Auteurs: Andrei Mikheev∗

No abstract was found

References : Aberdeen, John S., John D. Burger, David S.Day, Lynette Hirschman, PatriciaRobinson, and Marc Vilain. 1995. “Mitre:Description of the alembic system usedfor MUC-6.” In Proceedings of the SixthMessage Understanding Conference (MUC-6),Columbia, Maryland, November. MorganKaufmann.Baldwin, Breck, Christine Doran, JeffreyReynar, Michael Niv, Bangalore Srinivas,and Mark Wasson. 1997. “EAGLE: Anextensible architecture for generallinguistic engineering.” In Proceedings ofComputer-Assisted Information Searching onInternet (RIAO ’97), Montreal, June.Baum, Leonard E. and Ted Petrie. 1966.Statistical inference for probabilisticfunctions of finite Markov chains. Annalsof Mathematical Statistics 37:1559–1563.Bikel, Daniel, Scott Miller, RichardSchwartz, and Ralph Weischedel. 1997.“Nymble: A high performance learningname-finder.” In Proceedings of the FifthConference on Applied Natural LanguageProcessing (ANLP’97), pages 194–200.Washington, D.C., Morgan Kaufmann.Brill, Eric. 1995a. Transformation-basederror-driven learning and naturallanguage parsing: A case study inpart-of-speech tagging. ComputationalLinguistics 21(4):543–565.Brill, Eric. 1995b. “Unsupervised learning ofdisambiguation rules for part of speechtagging.” In David Yarovsky and KennethChurch, editors, Proceedings of the ThirdWorkshop on Very Large Corpora, pages1–13, Somerset, New Jersey. Associationfor Computational Linguistics.Burnage, Gavin. 1990. CELEX: A Guide forUsers. Centre for Lexical Information,Nijmegen, Netherlands.317MikheevPeriods, Capitalized Words, etc.Chinchor, Nancy. 1998. “Overview ofMUC-7.” In Seventh Message UnderstandingConference (MUC-7): Proceedings of aConference Held in Fairfax, April. MorganKaufmann.Church, Kenneth. 1988. “A stochastic partsprogram and noun-phrase parser forunrestricted text.” In Proceedings of theSecond ACL Conference on Applied NaturalLanguage Processing (ANLP’88), pages136–143, Austin, Texas.Church, Kenneth. 1995. “One term or two?”In SIGIR’95, Proceedings of the 18th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 310–318, Seattle,Washington, July. ACM Press.Clarkson, Philip and Anthony J. Robinson.1997. “Language model adaptation usingmixtures and an exponentially decayingcache.” In Proceedings IEEE InternationalConference on Speech and Signal Processing,Munich, Germany.Cucerzan, Silviu and David Yarowsky. 1999.“Language independent named entityrecognition combining morphological andcontextual evidence.” In Proceedings ofJoint SIGDAT Conference on EMNLP andVLC.Francis, W. Nelson and Henry Kucera. 1982.Frequency Analysis of English Usage: Lexiconand Grammar. Houghton Mifflin, NewYork.Gale, William, Kenneth Church, and DavidYarowsky. 1992. “One sense perdiscourse.” In Proceedings of the FourthDARPA Speech and Natural LanguageWorkshop, pages 233–237.Grefenstette, Gregory and Pasi Tapanainen.1994. “What is a word, what is asentence? Problems of tokenization.” InThe Proceedings of Third Conference onComputational Lexicography and TextResearch (COMPLEX’94), Budapest,Hungary.Krupka, George R. and Kevin Hausman.1998. Isoquest Inc.: Description of thenetowl extractor system as used forMUC-7. In Proceedings of the SeventhMessage Understanding Conference (MUC-7),Fairfax, VA. Morgan Kaufmann.Kuhn, Roland and Renato de Mori. 1998. Acache-based natural language model forspeech recognition. IEEE Transactions onPattern Analysis and Machine Intelligence12:570–583.Kupiec, Julian. 1992. Robust part-of-speechtagging using a hidden Markov model.Computer Speech and Language.Mani, Inderjeet and T. Richard MacMillan.1995. “Identifying unknown propernames in newswire text.” In B. Boguraevand J. Pustejovsky, editors, CorpusProcessing for Lexical Acquisition. MIT Press,Cambridge, Massachusetts, pages 41–59.Marcus, Mitchell, Mary Ann Marcinkiewicz,and Beatrice Santorini. 1993. Building alarge annotated corpus of English: ThePenn treebank. Computational Linguistics19(2):313–329.Mikheev, Andrei. 1997. Automatic ruleinduction for unknown word guessing.Computational Linguistics 23(3):405–423.Mikheev, Andrei. 1999. A knowledge-freemethod for capitalized worddisambiguation. In Proceedings of the 37thConference of the Association forComputational Linguistics (ACL’99), pages159–168, University of Maryland, CollegePark.Mikheev, Andrei. 2000. “Tagging sentenceboundaries.” In Proceedings of the FirstMeeting of the North American Chapter of theComputational Linguistics (NAACL’2000),pages 264–271, Seattle, Washington.Morgan Kaufmann.Mikheev, Andrei, Clair Grover, and ColinMatheson. 1998. TTT: Text Tokenisation Tool.Language Technology Group, Universityof Edinburgh. Available athttp://www.ltg.ed.ac.uk/software/ttt/index.html.Mikheev, Andrei, Clair Grover, and MarcMoens. 1998. Description of the ltgsystem used for MUC-7. In SeventhMessage Understanding Conference(MUC–7): Proceedings of a Conference Held inFairfax, Virginia. Morgan Kaufmann.Mikheev, Andrei and Liubov Liubushkina.1995. Russian morphology: Anengineering approach. Natural LanguageEngineering 1(3):235–260.Palmer, David D. and Marti A. Hearst. 1994.“Adaptive sentence boundarydisambiguation.” In Proceedings of theFourth ACL Conference on Applied NaturalLanguage Processing (ANLP’94), pages78–83, Stuttgart, Germany, October.Morgan Kaufmann.Palmer, David D. and Marti A. Hearst. 1997.Adaptive multilingual sentence boundarydisambiguation. Computational Linguistics23(2):241–269.Park, Youngja and Roy J. Byrd. 2001.“Hybrid text mining for findingabbreviations and their definitions.” InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMLP’01), pages 16–19, Washington,D.C. Morgan Kaufmann.Ratnaparkhi, Adwait. 1996. “A maximumentropy model for part-of-speech318Computational LinguisticsVolume 28, Number 3tagging.” In Proceedings of Conference onEmpirical Methods in Natural LanguageProcessing, pages 133–142, University ofPennsylvania, Philadelphia.Reynar, Jeffrey C. and Adwait Ratnaparkhi.1997. “A maximum entropy approach toidentifying sentence boundaries.” InProceedings of the Fifth ACL Conference onApplied Natural Language Processing(ANLP’97), pages 16–19. MorganKaufmann.Riley, Michael D. 1989. “Some applicationsof tree-based modeling to speech andlanguage indexing.” In Proceedings of theDARPA Speech and Natural LanguageWorkshop, pages 339–352. MorganKaufmann.Yarowsky, David. 1993. “One sense percollocation.” In Proceedings of ARPAHuman Language Technology Workshop ’93,pages 266–271, Princeton, New Jersey.Yarowsky, David. 1995. “Unsupervisedword sense disambiguation rivalingsupervised methods.” In Meeting of theAssociation for Computational Linguistics(ACL’95), pages 189–196.

Discussion : In this article we presented an approach that tackles three important aspects of text nor-malization: sentence boundary disambiguation, disambiguation of capitalized wordswhen they are used in positions where capitalization is expected, and identification ofabbreviations. The major distinctive features of our approach can be summarized asfollows:•We tackle the sentence boundary task only after we have fullydisambiguated the word on the left and the word on the right of apotential sentence boundary punctuation sign.•To disambiguate capitalized words and abbreviations, we useinformation distributed across the entire document rather than theirimmediate local context.•Our approach does not require manual rule construction or dataannotation for training. Instead, it relies on four word lists that can begenerated completely automatically from a raw (unlabeled) corpus.In this approach we do not try to resolve each ambiguous word occurrence individu-ally. Instead, the system scans the entire document for the contexts in which the wordsin question are used unambiguously, and this gives it grounds, acting by analogy, forresolving ambiguous contexts.We deliberately shaped our approach so that it largely does not rely on precom-piled statistics, because the most interesting events are inherently infrequent and henceare difficult to collect reliable statistics for. At the same time precompiled statisticswould be smoothed across multiple documents rather than targeted to a specific docu-ment. By collecting suggestive instances of usage for target words from each particulardocument on the fly, rather than relying on preacquired resources smoothed across theentire document collection, our approach is robust to domain shifts and new lexicaand closely targeted to each document.314Computational LinguisticsVolume 28, Number 3A significant advantage of this approach is that it can be targeted to new domainscompletely automatically, without human intervention. The four word lists that oursystem uses for its operation can be generated automatically from a raw corpus andrequire no human annotation. Although some SBD systems can be trained on relativelysmall sets of labeled examples, their performance in such cases is somewhat lower thantheir optimal performance. For instance, Palmer and Hearst (1997) report that the SATZsystem (decision tree variant) was trained on a set of about 800 labeled periods, whichcorresponds to a corpus of about 16,000 words. This is a relatively small training setthat can be manually marked in a few hours’ time. But the error rate (1.5%) of thedecision tree classifier trained on this small sample was about 50% higher than thatwhen trained on 6,000 labeled examples (1.0%).The performance of our system does not depend on the availability of labeledtraining examples. For its “training,” it uses a raw (unannotated in any way) corpusof texts. Although it needs such a corpus to be relatively large (a few hundred thousandwords), this is normally not a problem, since when the system is targeted to a newdomain, such a corpus is usually already available at no extra cost. Therefore there is notrade-off between the amount of human labor and the performance of the system. Thisnot only makes retargeting of such system easier but also enables it to be operationalin a completely autonomous way: it needs only to be pointed to texts from a newdomain, and then it can retarget itself automatically.Although the DCA requires two passes through a document, the simplicity of theunderlying algorithms makes it reasonably fast. It processes about 3,000 words persecond using a Pentium II 400 MHz processor. This includes identification of abbre-viations, disambiguation of capitalized words, and then disambiguation of sentenceboundaries. This is comparable to the speed of other preprocessing systems.3 The oper-ational speed is about 10% higher than the training speed because, apart from applyingthe system to the training corpus, training also involves collecting, thresholding, andsorting of the word lists—all done automatically but at extra time cost. Training onthe 300,000-word NYT text collection took about two minutes.Despite its simplicity, the performance of our approach was on the level withthe previously highest reported results on the same test collections. The error rateon sentence boundaries in the Brown corpus was not significantly worse than thelowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpusour system performed slightly better than the combination of the Alembic and SATZsystems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Althoughthese error rates seem to be very small, they are quite significant. Unlike general POStagging, in which it is unfair to expect an error rate of less than 2% because even humanannotators have a disagreement rate of about 3%, sentence boundaries are much lessambiguous (with a disagreement of about 1 in 5,000). This shows that an error rateof 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand,one error in 200 periods means that there is one error in every two documents in theBrown corpus and one error in every four documents in the WSJ corpus.With all its strong points, there are a number of restrictions to the proposed ap-proach. First, in its present form it is suitable only for processing of reasonably “well-behaved” texts that consistently use capitalization (mixed case) and do not containmuch noisy data. Thus, for instance, we do not expect our system to perform wellon single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their averagesentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha3000).315MikheevPeriods, Capitalized Words, etc.optical character reader–generated texts. We noted in Section 8 that very short doc-uments of one to three sentences also present a difficulty for our approach. This iswhere robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS taggerreported in Mikheev (2000), which do not heavily rely on word capitalization and arenot sensitive to document length, have an advantage.Our DCA uses information derived from the entire document and thus can beused as a complement to approaches based on the local context. When we incorpo-rated the DCA system into a POS tagger (Section 8), we measured a 30–35% cut in theerror rate on proper-name identification in comparison to DCA or the POS-taggingapproaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20%error rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corre-sponds to about a 20% cut in the error rate in comparison to DCA or the POS-taggingapproaches alone.We also investigated the portability of our approach to other languages and ob-tained encouraging results on a corpus of news in Russian. This strongly suggests thatthe DCA method can be applied to the majority of European languages, since theyshare the same principles of capitalization and word abbreviation. Obvious exceptions,though, are German and some Scandinavian languages in which capitalization is usedfor things other than proper-name and sentence start signaling. This does not mean,however, that the DCA in general is not suitable for preprocessing of German texts—itjust needs to be applied with different disambiguation clues.Initially the system described in this article was developed as a text normalizationmodule for a named-entity recognition system (Mikheev, Grover, and Moens 1998) thatparticipated in MUC-7. There the ability to identify proper names with high accuracyproved to be instrumental in enabling the entire system to achieve a very high level ofperformance. Since then this text normalization module has been used in several othersystems, and its ability to be adapted easily to new domains enabled rapid develop-ment of text analysis capabilities in medical, legal, and law enforcement domains.Appendix A: SBD Rule SetIn this section we present the rule set used by our system to assign potential sentenceboundary punctuation asFSPunctuation that signals end of sentenceAPPeriod that is part of abbreviationAFSPeriod that is part of abbreviation and signals end of sentenceThis rule set operates over tokens that are disambiguated as to whether or not theyare abbreviations and whether or not they are proper names. Tokens are categorizedinto overlapping sets as follows:NONENo token (end of input)ANYAny tokenANY-OR-NONEAny token or no token at allABBRToken that was disambiguated as “abbreviation”(Note: . . . Ellipsis is treated as an abbreviation too)Not ABBRNonpunctuation token that was disambiguated as “notabbreviation”CLOSE PUNCTClosing quotes, closing bracketsOPEN PUNCTOpening quotes, opening brackets316Computational LinguisticsVolume 28, Number 3PUNCTPunctuation token not CLOSE PUNCT or OPEN PUNCTor [.!?;]NUMNumberLOW COMMONLower-cased common wordCAP COMMONCapitalized word that was disambiguated as a common wordCAP PROPCapitalized word that was disambiguated as a proper namePROPER NAMEProper nameRule Setword-2 word-1FOCAL word+1word+2Assign ExampleANYNot ABBR[.?!]ANY-OR-NONEANY-OR-NONEFSbook.ANYCLOSE PUNCT [.?!]ANY-OR-NONEANY-OR-NONEFS).ABBR.[.?!]ANY-OR-NONEANY-OR-NONEFSTex.!ANYANY;CAP COMMONANY-OR-NONEFS; TheABBR.NONENONEAFSTex.EOFABBR.CAP COMMONANY-OR-NONEAFSTex. TheABBR.CLOSE PUNCTCAP COMMONAFSkg.) ThisABBR.OPEN PUNCTCAP COMMONAFSkg. (ThisABBR.CLOSE PUNCTCAP COMMONAFSkg.) (ThisOPEN PUNCTABBR.PUNCTANY-OR-NONEAPkg.,ABBR.[.?!]ANY-OR-NONEAPTex.!ABBR.LOW COMMON ANY-OR-NONEAPkg. thisABBR.CLOSE PUNCTLOW COMMON APkg.) thisABBR.OPEN PUNCTLOW COMMON APkg. (thisABBR.CLOSE PUNCTLOW COMMON APkg.) (thisOPEN PUNCTABBR.ABBR.APSen. Gen.ABBR.NUMANY-OR-NONEAPkg. 5ABBR.PROPER NAMEANY-OR-NONEAPDr. Smith

