<article>
  <discussion>In this article we presented an approach that tackles three important aspects of text nor-malization: sentence boundary disambiguation, disambiguation of capitalized wordswhen they are used in positions where capitalization is expected, and identification ofabbreviations. The major distinctive features of our approach can be summarized asfollows:&#8226;We tackle the sentence boundary task only after we have fullydisambiguated the word on the left and the word on the right of apotential sentence boundary punctuation sign.&#8226;To disambiguate capitalized words and abbreviations, we useinformation distributed across the entire document rather than theirimmediate local context.&#8226;Our approach does not require manual rule construction or dataannotation for training. Instead, it relies on four word lists that can begenerated completely automatically from a raw (unlabeled) corpus.In this approach we do not try to resolve each ambiguous word occurrence individu-ally. Instead, the system scans the entire document for the contexts in which the wordsin question are used unambiguously, and this gives it grounds, acting by analogy, forresolving ambiguous contexts.We deliberately shaped our approach so that it largely does not rely on precom-piled statistics, because the most interesting events are inherently infrequent and henceare difficult to collect reliable statistics for. At the same time precompiled statisticswould be smoothed across multiple documents rather than targeted to a specific docu-ment. By collecting suggestive instances of usage for target words from each particulardocument on the fly, rather than relying on preacquired resources smoothed across theentire document collection, our approach is robust to domain shifts and new lexicaand closely targeted to each document.314Computational LinguisticsVolume 28, Number 3A significant advantage of this approach is that it can be targeted to new domainscompletely automatically, without human intervention. The four word lists that oursystem uses for its operation can be generated automatically from a raw corpus andrequire no human annotation. Although some SBD systems can be trained on relativelysmall sets of labeled examples, their performance in such cases is somewhat lower thantheir optimal performance. For instance, Palmer and Hearst (1997) report that the SATZsystem (decision tree variant) was trained on a set of about 800 labeled periods, whichcorresponds to a corpus of about 16,000 words. This is a relatively small training setthat can be manually marked in a few hours&#8217; time. But the error rate (1.5%) of thedecision tree classifier trained on this small sample was about 50% higher than thatwhen trained on 6,000 labeled examples (1.0%).The performance of our system does not depend on the availability of labeledtraining examples. For its &#8220;training,&#8221; it uses a raw (unannotated in any way) corpusof texts. Although it needs such a corpus to be relatively large (a few hundred thousandwords), this is normally not a problem, since when the system is targeted to a newdomain, such a corpus is usually already available at no extra cost. Therefore there is notrade-off between the amount of human labor and the performance of the system. Thisnot only makes retargeting of such system easier but also enables it to be operationalin a completely autonomous way: it needs only to be pointed to texts from a newdomain, and then it can retarget itself automatically.Although the DCA requires two passes through a document, the simplicity of theunderlying algorithms makes it reasonably fast. It processes about 3,000 words persecond using a Pentium II 400 MHz processor. This includes identification of abbre-viations, disambiguation of capitalized words, and then disambiguation of sentenceboundaries. This is comparable to the speed of other preprocessing systems.3 The oper-ational speed is about 10% higher than the training speed because, apart from applyingthe system to the training corpus, training also involves collecting, thresholding, andsorting of the word lists&#8212;all done automatically but at extra time cost. Training onthe 300,000-word NYT text collection took about two minutes.Despite its simplicity, the performance of our approach was on the level withthe previously highest reported results on the same test collections. The error rateon sentence boundaries in the Brown corpus was not significantly worse than thelowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpusour system performed slightly better than the combination of the Alembic and SATZsystems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Althoughthese error rates seem to be very small, they are quite significant. Unlike general POStagging, in which it is unfair to expect an error rate of less than 2% because even humanannotators have a disagreement rate of about 3%, sentence boundaries are much lessambiguous (with a disagreement of about 1 in 5,000). This shows that an error rateof 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand,one error in 200 periods means that there is one error in every two documents in theBrown corpus and one error in every four documents in the WSJ corpus.With all its strong points, there are a number of restrictions to the proposed ap-proach. First, in its present form it is suitable only for processing of reasonably &#8220;well-behaved&#8221; texts that consistently use capitalization (mixed case) and do not containmuch noisy data. Thus, for instance, we do not expect our system to perform wellon single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their averagesentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha3000).315MikheevPeriods, Capitalized Words, etc.optical character reader&#8211;generated texts. We noted in Section 8 that very short doc-uments of one to three sentences also present a difficulty for our approach. This iswhere robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS taggerreported in Mikheev (2000), which do not heavily rely on word capitalization and arenot sensitive to document length, have an advantage.Our DCA uses information derived from the entire document and thus can beused as a complement to approaches based on the local context. When we incorpo-rated the DCA system into a POS tagger (Section 8), we measured a 30&#8211;35% cut in theerror rate on proper-name identification in comparison to DCA or the POS-taggingapproaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20%error rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corre-sponds to about a 20% cut in the error rate in comparison to DCA or the POS-taggingapproaches alone.We also investigated the portability of our approach to other languages and ob-tained encouraging results on a corpus of news in Russian. This strongly suggests thatthe DCA method can be applied to the majority of European languages, since theyshare the same principles of capitalization and word abbreviation. Obvious exceptions,though, are German and some Scandinavian languages in which capitalization is usedfor things other than proper-name and sentence start signaling. This does not mean,however, that the DCA in general is not suitable for preprocessing of German texts&#8212;itjust needs to be applied with different disambiguation clues.Initially the system described in this article was developed as a text normalizationmodule for a named-entity recognition system (Mikheev, Grover, and Moens 1998) thatparticipated in MUC-7. There the ability to identify proper names with high accuracyproved to be instrumental in enabling the entire system to achieve a very high level ofperformance. Since then this text normalization module has been used in several othersystems, and its ability to be adapted easily to new domains enabled rapid develop-ment of text analysis capabilities in medical, legal, and law enforcement domains.Appendix A: SBD Rule SetIn this section we present the rule set used by our system to assign potential sentenceboundary punctuation asFSPunctuation that signals end of sentenceAPPeriod that is part of abbreviationAFSPeriod that is part of abbreviation and signals end of sentenceThis rule set operates over tokens that are disambiguated as to whether or not theyare abbreviations and whether or not they are proper names. Tokens are categorizedinto overlapping sets as follows:NONENo token (end of input)ANYAny tokenANY-OR-NONEAny token or no token at allABBRToken that was disambiguated as &#8220;abbreviation&#8221;(Note: . . . Ellipsis is treated as an abbreviation too)Not ABBRNonpunctuation token that was disambiguated as &#8220;notabbreviation&#8221;CLOSE PUNCTClosing quotes, closing bracketsOPEN PUNCTOpening quotes, opening brackets316Computational LinguisticsVolume 28, Number 3PUNCTPunctuation token not CLOSE PUNCT or OPEN PUNCTor [.!?;]NUMNumberLOW COMMONLower-cased common wordCAP COMMONCapitalized word that was disambiguated as a common wordCAP PROPCapitalized word that was disambiguated as a proper namePROPER NAMEProper nameRule Setword-2 word-1FOCAL word+1word+2Assign ExampleANYNot ABBR[.?!]ANY-OR-NONEANY-OR-NONEFSbook.ANYCLOSE PUNCT [.?!]ANY-OR-NONEANY-OR-NONEFS).ABBR.[.?!]ANY-OR-NONEANY-OR-NONEFSTex.!ANYANY;CAP COMMONANY-OR-NONEFS; TheABBR.NONENONEAFSTex.EOFABBR.CAP COMMONANY-OR-NONEAFSTex. TheABBR.CLOSE PUNCTCAP COMMONAFSkg.) ThisABBR.OPEN PUNCTCAP COMMONAFSkg. (ThisABBR.CLOSE PUNCTCAP COMMONAFSkg.) (ThisOPEN PUNCTABBR.PUNCTANY-OR-NONEAPkg.,ABBR.[.?!]ANY-OR-NONEAPTex.!ABBR.LOW COMMON ANY-OR-NONEAPkg. thisABBR.CLOSE PUNCTLOW COMMON APkg.) thisABBR.OPEN PUNCTLOW COMMON APkg. (thisABBR.CLOSE PUNCTLOW COMMON APkg.) (thisOPEN PUNCTABBR.ABBR.APSen. Gen.ABBR.NUMANY-OR-NONEAPkg. 5ABBR.PROPER NAMEANY-OR-NONEAPDr. Smith</discussion>
</article>
