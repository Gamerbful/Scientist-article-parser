<article>
  <discussion>5.1RGA R and Fleiss&#8217; Kappa correlationIn Sect. 3 we described the WiSeBE score and how it relies on the RGAR valueto scale the performance of CT over RW . RGAR can intuitively be consider anagreement value over all elements of R. To test this hypothesis, we computedthe Pearson correlation coefficient (PCC) [21] between RGAR and the Fleiss&#8217;Kappa [4] of each video in the dataset (&#954;R).A linear correlation between RGAR and &#954;R can be observed in Table 6. Thisis confirmed by a PCC value equal to 0.890, which means a very strong positivelinear correlation between them.5.2F 1mean vs. W iSeBEResults form Table 5 may give an idea that WiSeBE is just an scaled F1mean.While it is true that they show a linear correlation, WiSeBE may produce a128C.-E. Gonz&#180;alez-Gallardo and J.-M. Torres-MorenoTable 5. WiSeBE evaluationTranscriptSystem F1mean F1RWRGARWiSeBEv1S10.4320.4950.6910.342S20.4800.5130.354v2S10.5780.6590.6880.453S20.5490.5950.409v3S10.2700.3030.6840.207S20.3250.4000.274v4S10.5050.5930.5780.342S20.7350.8000.462v5S10.5920.6140.7670.471S20.4990.5000.383v6S10.4430.5500.5410.298S20.4570.5350.289v7S10.5180.5920.6170.366S20.5390.6060.374v8S10.4290.4940.5250.259S20.4870.5080.267v9S10.4590.5690.6040.344S20.5410.6670.403v10S10.5820.5810.6190.359S20.4870.5450.338Mean scores S10.4810.5450.6310.344S20.5100.5670.355Table 6. Agreement within datasetAgreement metricv1v2v3v4v5v6v7v8v9v10RGAR0.6910.6880.6840.5780.7670.5410.6170.5250.6040.619&#954;R0.7760.6970.7570.6960.8390.6300.7430.6550.7040.718different system ranking than F1mean given the integral multi-reference principleit follows. However, what we consider the most profitable about WiSeBE is thetwofold inclusion of all available references it performs. First, the construction ofRW to provide a more inclusive reference against to whom be evaluated and then,the computation of RGAR, which scales the result depending of the agreementbetween references.WiSeBE: Window-Based Sentence Boundary Evaluation1296</discussion>
</article>
