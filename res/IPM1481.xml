<article>
  <preamble>IPM1481.pdf</preamble>
  <titre></titre>
  <auteurs>
    <auteur></auteur>
    <affiliation>No affiliation</affiliation>
  </auteurs>
  <abstract>Automatic summarization is useful to cope with ever increasing volumes of information. An abstract is, by far, the mostconcrete and recognized kind of text condensation. However, the CV is already a kind of summary, with a very importantstructure. We suspect that the filtering system of automatic summarization may not be useful in this case. Since the CL isin free text, we used CORTEX (Torres-Moreno, St-Onge, Gagnon, El-B&#232;ze, &amp; Bellot, 2009, 2001), an efficient state-of-art summa-rization system, in order to retain the more informative segments of the CL.Each document of the application is transmitted to the CORTEX system which provides a summary based on the requestedsize. CORTEX is a document extract summarization system using an optimal decision algorithm that combines several metrics.These metrics result from processing statistical and informational algorithms on the document vector space representation.Fig. 2 presents an overview of the system.The idea is to represent the text in an appropriate vectorial space and apply numeric processings to it. In order to reducecomplexity, a pre-processing of the document is performed: words are filtered, lemmatized, and stemmed. Based on theterms that remain in the text after filtering, a frequency matrix c is built in the following way: Each element cli of this matrixrepresents the number of occurrences of the word i in the sentence l.13 JobFinder (http://www.jobfinder.com).1128R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copyc &#188;c11c12. . .c1i. . .c1NLc21c22. . .c2i. . .c2NL..................cl1cl2. . .cli. . .clNL..................cNS1cNS2. . .cNSi. . .cNSNL2666666666666437777777777775;cli 2 f0; 1; 2; . . .g&#240;5&#222;Another matrix n, called a binary virtual or presence matrix, is defined as:nli &#188;1if cli &#8211; 00elsewhere()&#240;6&#222;Each line of these matrices represents a sentence of the text. Matrices c and cT are the frequency matrix of the sentencesand frequency matrix of the titles respectively.The CORTEX system can use up to C = 11 metrics (Torres-Moreno, Velazquez-Morales, &amp; Meunier, 2002) to evaluate the sen-tence&#8217;s relevance.The system scores each sentence with a decision algorithm which relies on the normalized metrics. Two averages are cal-culated, a positive ks &gt; 0.5, and a negative ks &lt; 0.5 tendency (the case ks = 0.5 is ignored). The following algorithm combinesthe vote of each metric:Ps a &#188; PCv&#188;1kvs&#65533;&#65533;&#65533;&#65533; &#65533; 0:5&#65533;&#65533;;kvs&#65533;&#65533;&#65533;&#65533; &gt; 0:5Psb &#188; PCv&#188;10:5 &#65533; kvs&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;;kvs&#65533;&#65533;&#65533;&#65533; &lt; 0:5Cis the number of metrics and v is the index of the metrics. The value given to each sentence s is calculated with:ifPs a &gt; Psb&#65533;&#65533;then Scorecortexs&#188; 0:5 &#254; Psa=C: retain selse Scorecortexs&#188; 0:5 &#65533; Psb=C: not retain sFig. 2. CORTEX overview.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351129Author's personal copyThe sentences are then ranked according to the obtained values. Depending on the desired compression rate, the sortedsentences will be used to produce the summary. The CORTEX system is applied to each document (Cover Letter) and a sum-mary is generated by concatenating high-scoring sentences. We generated several abstracts with a variable compression rate(5%, 10%, 20%, . . ., 50%, 75% of the size of the documents, in sentences) in order to test the impact of our powerful filter on theE-Gen system. The entire process chain is illustrated in Fig. 1. The best compression rates are generally with 30% (Torres-Moreno et al., 2009). The results are presented in Section 5.3.5. ExperimentsWe selected a data subset from Aktor&#8217;s database composed of 1917 candidates. This subset is called the Mission Corpus. Ithas a size of 10 MB of raw texts and contains 1,375,000 words. The Mission Corpus is composed of a set of 12 job offers cov-ering various themes (jobs in accountancy, business, computer science, etc.) and their candidates. Each Job Offer is associatedwith at least six candidates identified as relevant. As described in Kessler et al. (2008a), each document is segmented to keepthe relevant parts (we remove the description of the company (D) for the job offer). Each candidate answer is tagged as rel-evant or irrelevant. A relevant value corresponds to a potential candidate for a specific job chosen by the recruiting consul-tant. An irrelevant value is associated with an unsuitable candidate for the job (this is a decision made by the manager of ahuman resources company). Our study was conducted on French job offers because the French market represents Aktor&#8217;smain activity. Table 1 shows a few statistics about the Mission Corpus.5.1. Example of CL summariesFig. 3 presents14 an example of an original Cover Letter and Fig. 4. Its corresponding summary15generated by the CORTEX sys-tem with a 30% compression rate (in number of sentences).All the documents of Mission Corpus were previously made anonymous. We observe that the original CL contains anumber of useless information for ranking, such as addresses, phone numbers or form of address at the beginning orend of the letter. The last part of the CL is generally as &#8216;&#8216;Yours faithfully&#8217;&#8217;, &#8216;&#8216;Yours sincerely&#8217;&#8217;, &#8216;&#8216;Best regards&#8217;&#8217;, all of whichrepresent irrelevant information. We further observe in Fig. 4 that the summary obtained with CORTEX removes all thisinformation.5.2. Experimental protocolWe measured the similarity between a job offer and its candidate&#8217;s responses. These measures (Section 4.1.2) rank thecandidate&#8217;s answers by computing a similarity between a job offer and the associated candidate answers. We use theROC curves to evaluate the quality of the ranking obtained. ROC curves (Ferri, Flach, &amp; Hernandez-Orallo, 2002) come fromthe field of signal processing. They are used in medicine to evaluate the validity of diagnostic tests. In our case, ROC curvesshow the rate of irrelevant candidate answers on the X-axis and the rate of relevant candidate answers on the Y-axis. The14 Pierre ASPRE26 years old19 Verdun street 92870 Vannes06-06-06-06-06.Subject: collaboration offerVannes, November 27th, 2008Dear Sir,The Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing profitability. With his legal knowledge in taxand social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices andknowing how to deal with the late settlement of invoices.Therefore I offer my skills. They allow me to:&#8211; Manage with rigueur the accounts of a company.&#8211; Ensure legal compliance activities (payroll, tax billing etc.).&#8211; Provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis.I suggest we meet to discuss all the terms of our future cooperation.I look forward to hearing from you.Best regards.Pierre ASPRE.15 Pierre ASPRESubject: collaboration offerThe Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing profitability. With his legal knowledge in taxand social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices andknowing how to deal with late settlement of invoices.&#8211; ensure legal compliance activities (payroll, tax billing etc.).&#8211; provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis.1130R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copyArea Under the Curve (AUC) can be interpreted as the effectiveness of a measurement of interest. In the case of candidateanswers ranking, a perfect ROC curve corresponds to obtaining all relevant candidate answers at the beginning of the listand all irrelevant ones at the end. This situation corresponds to AUC = 1. The diagonal line corresponds to the performanceof a random system, progress of the rate of relevant candidates being accompanied by an equivalent degradation in the rateof irrelevant candidates. This situation corresponds to AUC = 0.5, as explained in Fawcett (2006). An effective measurementFig. 3. Example of full Cover Letter.Table 1Mission corpus statistics.NumberJob titleNumber of candidate answersNumber ofRelevantIrrelevant34861Sales engineer40142631702Accountant, department suppliers55233233633Sales engineer65184734865Accountant assistant67105734783Accountant assistant108999337463 chefs116605633553Trade commissioner1171710033725Urban sales consultant118437531022Recruitment assistant2212819331274Accountant assistant junior2242619834119Sales assistant2571024731767Accountant assistant junior43751386Total19173231594Fig. 4. Summary of Cover Letter (see Fig. 3) at a 30% compression rate.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351131Author's personal copyof interest to order candidate&#8217;s answers consists in obtaining the highest AUC value. This is strictly equivalent to minimizingthe sum of the ranks of the relevant candidate&#8217;s answers. ROC curves are resistant to imbalance (for example, an imbalancein the number of positive and negative examples) (Roche &amp; Kodratoff, 2006). For each job offer, we evaluated the quality ofthe ranking obtained by this method. Candidate answers considered are only those composed of CV and CL.5.3. ResultsIn this section, we present the results obtained by combining the CORTEX system with the E-Gen ranking application. CORTEXwas used as an additional filter which generates a summary of each document before E-Gen evaluation. We keep the struc-ture of data for job offers as described in Kessler et al. (2008a). A job offer is composed of a Description (D), a Title (T), aMission (M), and a Profile (P). For these experiments, we use two combinations of a job offer content, keeping only Title, Mis-sion, Profile (TMP) and all information of a job offer (DTMP). Results are presented in Tables 2 and 3. Each column presents apart of the application with different sizes of summaries for each line (75%, 50%, . . ., 5%). Full text is a result obtained with100% of the document and was published previously in Kessler et al. (2008a, 2009).Table 2 presents results obtained for each part of the application separately. We observe that AUC of CVs remains belowthe baseline whatever the percentage of compression. We notice however a gradual decrease in AUC scores depending on thepercentage of compression. We explain this by the fact that a CV is already a summary of the most important informationabout the candidates and thereby attempting to summarize degrades final results. We apply the same process with coverletters. Performance is still low overall for CLs in comparison with CVs, however, there is a slight increase in AUC scores witha compression rate of 30%. We explain these results by particular information contained in a cover letter such as the form ofaddress at the beginning or end of the letter (see Fig. 4) which are noise for the ranking system of E-Gen. Results with TMPsegmentation (i.e. conserving only Title, Mission, and Profile of job offer) are of better quality.Table 3 presents the results obtained by combining both parts of the application. Full text values are computed with thewhole documents of the application. The first two columns show the results obtained by combining the summary of the CVand the CL. We observe again a deterioration in the results when trying to summarize the CV. Even if results are lower, itshould be noted, however, that the best score is again obtained at 30%. The last two columns present the results with a sum-marized CL and the full CV. We observe an overall improvement of the AUC score and the best results with a compressionrate of 30% of the Cover Letter.Next step is to combine summaries of the cover letter, which suppresses noise and enriches the offer with the RelevanceFeedback process. Table 4 presents the results obtained with different sizes of Relevance Feedback (RF1 corresponds to oneapplication added to the job offer, RF2 two applications added to the job offer, etc.). Each application added with the rele-vance feedback process consists in a full CV and a summary of the cover letter with a compression rate of 30%. A randomdistribution of applications produces an AUC approximately at 0.5 like explained in Fawcett (2006). We compare ISMIS Resultwith those obtained using a summary of the cover letter. Each test is carried out 100 times with a random distribution ofrelevant applications for Relevance Feedback. Then we compute an average of AUC scores obtained (the curve shows theTable 2Results of CL or CV according to the compression rate of Cortex and part of job offer (with or without Description part).CORTEX compression rate (%)CV + DTMPCV + TMPCL + DTMPCL + TMP100 (full text)0.6220.6480.5670.560750.5650.5750.5630.556500.5580.5690.5530.560400.5520.5650.5610.565300.5490.5600.5690.571200.5200.5580.5640.566100.5590.5590.5430.55450.5500.5420.5210.523Table 3Results for CV and cover letter according to the compression rate.CORTEX compression rate (%)CV and CL summariesFull CV and CL summaryDTMPTMPDTMPTMP100 (full text)0.6340.6420.6340.642750.5210.5810.6390.641500.5560.5510.6430.649400.5440.5680.6430.651300.5700.5870.6460.653200.5690.5330.6410.652100.5640.5340.6310.64550.5460.5470.6380.6491132R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copyaverage for each size). In fact, we compute the Residual Ranking (Billerbeck &amp; Zobel, 2006): Documents that are used for Rel-evance Feedback are removed from the collection before ranking with the reformulated query. We assume that the Rele-vance Feedback process would behave as a reinforcement learning (Sutton &amp; Barto, 1998) but it is impossible toexperiment RFn with n &gt; 6 with this corpus because the number of relevant candidates is too small for some job offers(see Table 1). We observe a slight improvement in results for almost any size of Relevance Feedback. We are conscious thatthe performance gain is low, however, it confirms previous results on the Cover Letter. Fig. 5 shows this improvement. Thisfigure confirms that the addition of just one relevant candidate (RF1) enables the AUC value to be enhanced (i.e. an improve-ment of 0.5&#8211;1.2%). This Relevance Feedback (i.e. RF1) is not very time-consuming for the expert.Fig. 6 shows detailed results of one test. For clarity reasons, we present only 3 of the 12 jobs of our dataset in order tocompare results with and without CORTEX (for each job, RFC are AUC scores with CORTEX and RF without CORTEX).For standard system, we observe a positive progress from 1% to 10% for 10 jobs between RF0 and RF1 (e.g. five jobs havean improvement between 5% and 10%). Note that between RF0 and RF6, 6 jobs have a significant positive progress between10% and 12%. The combination of the E-Gen and CORTEX systems improve standard system results for five jobs from 1% to 5%between RF0 and RF1. Between RF0 and RF6, the Cortex version improves E-Gen&#8217;s results for eight jobs from 1% to 5%.The study of the results shows that job offer 31702 contains some relevant applications with a bad labeling (CV are la-beled CL and CL are only a hyperlink to a CV). The reduction of information on the main document of the application leadsthe system version using summaries to degrade the AUC scores. Job offer 34861 shows a good improvement with each size ofrelevance feedback (RF0:0.65, RF1:0.70, RF6:0.73) and with CORTEX (RF0:0.68, RF1:0.72, RF6:0.79). The detailed study of re-sults shows that job offer 33746 contains some empty applications labeled relevant. This leads the system with and withoutCORTEX to degrade final results. In the same way, an application added without CL explains the identical score in RF2 betweenRF and RFC for job offer 31274.6. Conclusion and future workJob offer processing is a difficult and highly subjective task. The retrieval of relevant information concerning job descrip-tions and skills is not a trivial task (Loth et al., 2010) and results on this type of document have been quite low (Clech &amp;Table 4Comparison of AUC score for each size of Relevance Feedback with CORTEX summarization system.Size of Relevance FeedbackISMIS resultFull CV and CL summary 30% compression rateRandom distribution0.5000.500RF00.6420.653RF10.6540.658RF20.6570.659RF30.6590.661RF40.6590.659RF50.6600.662RF60.6610.663Fig. 5. Results of Relevance Feedback with and without summaries of CL.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351133Author's personal copyZighed, 2003). The information we use in this kind of process is not well formated in natural language, but follows a conven-tional structure. This paper deals with the CORTEX summarizer and the E-Gen system for processing job offers. E-Gen assists anemployer in the recruitment task. This paper focuses on candidate answers to job offers. We rank the candidate answers byusing different similarity measures and different document representations in a vector space model. We use a process of rel-evance feedback to perform reinforcement learning, whereby each new application added to the process assists in the deci-sion-making. We choose to evaluate the quality of our approaches by computing Area Under the Curve.CORTEX is asummarization system using an optimal decision algorithm that combines several metrics. We present the results obtainedby combining both systems. AUC obtained with summarized cover letter at 30% of compression size and a full CV shows aslight improvement in the results. As future work, we plan to apply other techniques, such as finding discriminant features ofirrelevant applications using the Rocchio algorithm (Rocchio, 1971), weighting the different parts of an application, etc. inorder to improve results. We also plan to use a categorization of jobs to take into consideration similar jobs, such as &#8217;&#8217;devel-oper&#8217;&#8217; and &#8216;&#8216;programmer&#8217;&#8217;. Finally we propose to measure the CV quality by building an evaluation on an Internet portal. Ouraim with this evaluation is to present a job-seeker with a list of the most suitable job ads according to his profile.AcknowledgementsAuthors thank Richard James, V&#233;ronique Moriceau, Andr&#233; Bittar, ANRT (Agence Nationale de la Recherche Technologique)and Aktor Interactive that partially supported this work.ReferencesAudras, I., &amp; Ganascia, J.-G. (2006). Apprentissage du frantais langue TtrangFre et TALN: Analyses de corpus Tcrits a l&#8217;aide d&#8217;outils d&#8217;extraction automatiquedu langage. In J.-M. Viprey (Ed.), 8Fmes JournTes d&#8217;Analyse de DonnTes Textuelles (pp. 67&#8211;78). Univ. de Franche ComtT, Besanton 2006.Bellot, P., &amp; El-B&#232;ze, M. (2001). Classification et segmentation de textes par arbres de dTcision. In TSI (Vol. 20, pp. 107&#8211;134). HermFs.Ben Abdessalem Karaa, W. (2009). Web-based recruiting: A framework for cvs handling. In Second international conference on web and informationtechnologies &#8216;&#8216;ICWIT&#8217;09&#8217;&#8217;, kerkennah Island, Sfax, Tunisia, June 12&#8211;14 (pp. 395&#8211;406).Bernstein, A., Kaufmann, E., Kiefer, C., &amp; Bnrki, C. (2005). Simpack: A generic java library for similarity measures in ontologies. Tech. rep., University of ZurichDepartment of Informatics.Billerbeck, B., &amp; Zobel, J. (2006). Efficient query expansion with auxiliary data structures. Information Systems, 31(7), 573&#8211;584.Boudin, F., &amp; Torres Moreno, J. M. (2007). Neo-cortex: A performant user-oriented multi-document summarization system. In CICLing (pp. 551&#8211;562).Bourse, M., LeclFre, M., Morin, E., &amp; Trichet, F. (2004). Human resource management and semantic web technologies. In ICTTA 2004 Damascus Syria (pp. 641&#8211;642).Cazalens, S., &amp; Lamarre, P. (2001). An organization of internet agents based on a hierarchy of information domains. In Proceedings MAAMAW&#8217;2001, Annecy,France (pp. 573&#8211;584).Clech, J., &amp; Zighed, D. A. (2003). Data mining et analyse des cv: une exp&#233;rience et des perspectives. In EGC&#8217;03 Revue des Sciences et Technologies del&#8217;Information (Vol. 17, pp. 83&#8211;92). Lyon.Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F. M., Mongiello, M., &amp; Mottola, M. (2003). A formal approach to ontology-based semantic match of skillsdescriptions. Journal of Universal Computer Science, Special issue on Skills Management, 9, 1437&#8211;1454.Dorn, J., &amp; Naz, T. (2007). Meta-search in human resource management. In Proceedings of 4th international conference on knowledge systems ICKS&#8217;07Bangkok,Thailand (pp. 105&#8211;110).Enrica, A., &amp; Iezzi, D. F. (2006). Recruitment via web and information technology: A model for ranking the competences in job market. In JADT&#8217;2006,Besanton, France (pp. 79&#8211;88).Fan, R.-E., Chen, P.-H., &amp; Lin, C.-J. (2005). Working set selection using the second order information for training SVM. Journal of Machine Learning Research,1889&#8211;1918.</abstract>
  <introduction>The evolution of the job market has resulted in that traditional methods of recruitment becoming insufficient. TheInternet has introduced a new way of managing human resources. Theoretically, shifting job search and recruitment activ-ities to the Internet improves the quality of job matching by reducing search costs, increasing contact opportunities andrationalizing the screening process of job applicants (Marchal, Mellet, &amp; Rieucau, 2007). Over the last few years, there hasbeen a significant expansion of online recruitment (e.g. August 2003: 177,000 job offers, May 2008: 500,000 job offers).1The Internet has become essential in this process because it allows a better flow of information, either through job searchsites or by e-mail exchanges. Nowadays, job seekers can send their curriculum vitae (CV) directly to companies (by e-mailor uploaded to dedicated servers on the Web). The job search task is becoming easier and less time consuming. The Internetmakes every user a potential job seeker. Employees may be constantly in search of new career opportunities and job candi-dates may provide more interaction than can be managed efficiently by companies (Bourse, LeclFre, Morin, &amp; Trichet, 2004).As intellectual capital has become one of the most strategic assets of successful organizations in the last decade, the capa-bility of managing people&#8217;s expertise, skills and experience represents a key factor in facing up to the increasing competitive-ness of the global market (Colucci et al., 2003). Even though a browser has become a universal and easy tool for users, they0306-4573/$ - see front matter &#65533; 2012 Elsevier Ltd. All rights reserved.http://dx.doi.org/10.1016/j.ipm.2012.03.002&#8657; Corresponding author.E-mail addresses: remy.kessler@univ-avignon.fr (R. Kessler), nicolas.bechet@inria.fr (N. B&#233;chet), mathieu.roche@lirmm.fr (M. Roche), juan-manuel.tor-res@univ-avignon.fr (J.-M. Torres-Moreno), marc.elbeze@univ-avignon.fr (M. El-B&#232;ze).1 http://www.keljob.com.Information Processing and Management 48 (2012) 1124&#8211;1135Contents lists available at SciVerse ScienceDirectInformation Processing and Managementjournal homepage: www.elsevier.com/locate/infopromanAuthor's personal copyfrequently have to enter data into Web forms from paper sources and the need to &#8216;&#8216;copy and paste&#8217;&#8217; data between differentapplications is symptomatic of the issues of data integration. In this context, electronic recruitment tends to automate match-ing between the published information about the candidates and job offers. The Laboratoire Informatique d&#8217;Avignon (LIA),2 theLaboratoire d&#8217;Informatique, de Robotique et de Micro&#233;lectronique de Montpellier(LIRMM),3 and Aktor Interactive4 are developingthe E-Gen system to resolve this issue. E-Gen is a Natural Language Processing (NLP) and Information Retrieval (IR) systemcomposed of three main modules:1. The first one extracts the information from a corpus of e-mails of job offers from Aktor&#8217;s database.2. The second module analyses the candidate&#8217;s answers (i.e. splitting e-mails into cover letter (CL) and curriculum vitae).3. The third module analyses and computes a relevant ranking of the candidate&#8217;s answers.Our first work (Kessler, Torres-Moreno, &amp; El-B&#232;ze, 2007) presented the first module: the identification of different partsof a job offer and the extraction of relevant information (type of contract, salary, localization, etc.). The second moduleanalyses the content of a candidate&#8217;s e-mail, using a combination of rules and machine learning methods (Support VectorMachines, SVM) and was presented in Kessler, Torres-Moreno, and El-B&#232;ze (2008b). Furthermore, it separates the distinctparts of CV and CL with a precision of 0.98 and a recall of 0.96. Reading a large number of candidate answers for a job is avery time consuming task for a recruiting consultant. In order to facilitate this task, we propose a system capable of pro-viding an initial evaluation of candidate answers according to various criteria. We do not seek the best or even a good can-didate as no scoring is involved, but simply a candidate who has a close application to those already selected. Our previouswork (Kessler, B&#233;chet, Roche, El-B&#232;ze, &amp; Torres-Moreno, 2009) presented an approach based on a process of relevance feed-back, permitting a reinforcement learning (Sutton &amp; Barto, 1998). In this paper, we present an original combination of theE-Gen and CORTEX systems. Each document contains a number of additional information, present in many applications andwhich is partially removed by classical pre-processing. Each application added by the process of relevance feedback addsrelevant information but also multiplies additional information. CORTEX allows us to filter these sentences and keep only themost relevant sentences at the evaluation step. Some related studies are briefly discussed in Section 2. Section 3 shows ageneral system overview. In Section 4, we describe the E-Gen pre-processing task, the strategy used to rank the candidateanswers with relevance feedback and the coupling of E-Gen with the CORTEX summarization system. In Section 5, we presentstatistics about the textual corpus, experimental protocol, an example of CL summary generated by CORTEX, and severalresults.</introduction>
  <corp>Many approaches have been proposed in the literature to reduce the costly and tedious task of managing human re-sources. Candidate answers to a job offer come as ad hoc documents, and require semantic approaches to analyse them.The BONOM system is based on an indexing method (Morin, LeclFre, &amp; Trichet, 2004; Cazalens &amp; Lamarre, 2001). Thismethod consists in using distributional attributes of documents to locate each part for the final indexation of thedocument.A semantic-based method to select candidate answers and to discuss the economic impacts on the German governmentwas proposed by Tolksdorf, Mocho, Heese, Oldakowski, and Christian (2006). In the same way (Gorenak &amp; Mlaker KaF, 2010),perform a comparison between Slovenian, German, and British online job advertisements (ads). More recently (Marchal etal., 2007), present a comparison between French and English job search sites and newspapers as well as the various short-coming of current matching systems. They propose a comparative analysis of job offers posted on the Internet with thoseposted in newspapers and they observe that search engine toolkits have a considerable impact on ad content which is gen-erally more standardized and quantified than before.Mocho, Paslaru, and Simperl (2006) discuss the relevance of a common ontology (HR ontology) to work efficiently withthis kind of document. Using the same model (Dorn &amp; Naz, 2007), outline a HR-XML based prototype dedicated to the jobsearch task. The prototype selects and favors relevant information (paycheck, topic, abilities, etc.) from many job-servicewebsites, such as Jobs.net, aftercollege.com, Directjobs.com, etc. Bourse et al. (2004) describe an efficient modeland a management tool used for the selection of candidate-answers. They propose a prototype job portal which uses seman-tically annotated job offers and applicants to obtain a more accurate job search with query approximation.The limitations of current systems for automatic selection of candidate answers are presented in Rafter, Bradley, and Smyt(2000). They propose a system based on collaborative filters (ACF) to automatically select profiles of candidate answers onthe JobFinder website. Enrica and Iezzi (2006) present a model for ranking skills in the field of information technology in Italywith multidimensional scaling and cluster analysis. In the same way, Colucci et al. (2003) present a semantic based approachto the issue of skills detection in an ontology supported framework. Based on Description Logics formalization and reasoning,they propose a skill matching approach with contradiction matches and partial matches between skill profiles. Loth et al.2 http://www.lia.univ-avignon.fr.3 http://www.lirmm.fr.4 A French recruitment agency specialized in recruiting on the internet, (http://www.aktor.fr).R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351125Author's personal copy(2010) combine, through the SIRE project (Semantics-Internet-Recruitment-Employment) a linguistic approach and machinelearning methods to perform an extraction of key terms of job ads in order to improve the categorization of each job offer.The study of the most relevant document &#8211; the CV &#8211; to use it automatically has been a major subject of research. BenAbdessalem Karaa (2009) presents a system for analyzing and structuring CVs with an extension of General Architectureof Text Engineering (GATE5). They obtain good results in precision/recall for each part of the document (personal information,experience, skill, and so forth) on a small corpus of CVs in French. Yahiaoui, Boufa&#305;&#168;da, and Pri&#233; (2006) provide a semantic ap-proach to generating some annotations of CVs and job offers with the help of a specialized ontology to match graduates and thelevel of a job offer. They present interesting results on a sample of data. Clech and Zighed (2003) propose a data mining ap-proach. Their aim is to build automats which recognize CV topologies and candidate/job offer profiles. A first step differentiatesthe CV of employed executives from other CV. They use a specific term extraction to obtain a categorization with the C4.5 deci-sion tree algorithm (Quilan, 1993). This method focuses on the specificity of selected terms or concepts, such as education levelor relevant abilities, to build a classifier. The results of this method are still poor (an accuracy between 0.5&#8211;0.6 of correctly cat-egorized CV). Roche and Kodratoff (2006) and Roche and Prince (2008) have made a terminology study of corpus composed ofCVs (of the Vediorbis company (http://www.vediorbis.com)). Their approach extracts collocations from a CV corpus based onsyntactic patterns such as Noun-Noun, Adjective-Noun, etc. Then, these collocations are ranked according to relevance to builda specialized ontology.There are few studies on the treatment of the cover letter. Audras and Ganascia (2006) use cover letters to detect theusual errors in the field of acquisition of written French as a foreign language. The approach proposed is the detection ofsyntactic patterns particular to a group of learners, and which are absent or little used among native speakers. The studyfocuses in part on cover letter writing. Among the innovative solutions on the market, Twitter6 has launched the job searchsite http://www.twitterjobsearch.com based on the concept of short messages (less than 140 characters) and ZaPoint7 with anoriginal solution, SkillsMapper, which transforms each CV into graphic format with various curves (training, education, etc.). Inthis paper, we present an approach to the application ranking by using a combination of similarity measures, relevance feedbackand summaries of a CV and CL. Our approach is distinguished from other work by a purely statistical approach as well as rein-forcement learning through the process of relevance feedback.3. System overviewNowadays technology proposes new approaches to the online employment market. E-Gen is a system which meets thischallenge as fast and judiciously as possible. We chose emails as the input format, which is the most frequent mode of com-munication in this field. An e-mail inbox receives messages sometimes with an attached file containing the job offer. When ajob offer is published online, a particular segmentation is required by the job search sites. Firstly, the job offer language isidentified by using n-grams. Then, E-Gen parses the e-mail, splits the job offer into thematic segments, and retrieves relevantinformation (contract, salary, starting date, location, etc.) to generate an XML document for the job offer. Subsequently, afiltering and lemmatisation process is applied to the text, and is represented in a vector space model (VSM). A categorizationof text segments (preamble, skills or profile, mission) is obtained by using a SVM classifier (Fan, Chen, &amp; Lin, 2005). This pre-liminary classification is then transmitted to a &#8216;&#8216;corrective&#8217;&#8217; post-process which improves the quality of the solution (Module1, described in Kessler et al., 2007). Preliminary experiments showed that segment categorization without segment positionin job posting is not enough and may be a source of errors. In order to avoid this kind of error, we have decided to considereach job posting as produced by a succession of states in a Markov machine and we have applied a post-processing, based onthe Viterbi algorithm (Viterbi, 1967). During the publication of a job offer, Aktor generates a temporary e-mail address forapplying to the job. Each e-mail is redirected to human resources software (Gestmax8) to be read by a recruiting consultant.At this step, E-Gen analyses the candidate&#8217;s answers to identify each part of the application and extracts the text from the e-mailand attached files (by using wvWare9 and pdftotext10).After a pre-processing task, we use a combination of rules and machine learning methods to separate each distinct part(CV or CL). We use a vector representation of each document with a label (CV or CL). With a learning set of 2.000 documentsof each type, the system gets very good performance (F-score between 0.95 and 0.98). This process (Module 2 represented bythe lowest box in Fig. 1) is more fully described in Kessler et al. (2008b). Once the CL and CV have been identified, the CORTEXsystem is applied to each document (Cover Letter and CV) and a summary is generated by concatenating high-scoring sen-tences. Afterwards, E-Gen performs an automated profiling of this application by using measures of similarity and a smallnumber of applications that have been previously validated as relevant by a recruitment consultant (Module 3). The wholechain is summarized in Fig. 1.5 http://gate.ac.uk/.6 http://twitter.com.7 http://www.zapoint.com.8 http://www.gestmax.fr.9 http://wvware.sourceforge.net.10 http://www.bluem.net/downloads/pdftotext_en.1126R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copy4. Coupling E-Gen profiling module and the CORTEX system4.1. E-Gen profiling module4.1.1. Linguistic pre-processingFirstly, we remove information such as e-mail adresses, the names of candidates, addresses, names of cities in order toensure that the applications become anonymous. Then, classic pre-processing is applied to textual information (job offer,CV, and CL). French accents are deleted and capital letters are converted to lower case. This pre-processing task is performedto obtain a representation well suited for the Vector Space Model (VSM). In order to avoid the introduction of noise into themodels, the following items are also deleted: verbs and functional words (to be, to have, to need, etc.), common expressionswith a stop word11 list (for example, that is, each of, etc.), numbers (in numeric and/or textual format), symbols such as &#8216;&#8216;$&#8217;&#8217;, &#8216;&#8216;#&#8217;&#8217;,&#8216;&#8216;&#8260;&#8217;&#8217;. Finally, lemmatisation12 is performed to significantly reduce the size of the lexicon. All these processes allow us to repre-sent the collection of documents through the bag-of-words paradigm (a matrix of frequencies of terms (columns) for each can-didate answer (rows)). To improve filtering, we tried parsing applications with different significant terms (like &#8216;&#8216;PersonalInformation&#8217;&#8217;, &#8216;&#8216;Education&#8217;&#8217;, &#8217;&#8217;Work Experience&#8217;&#8217;, etc.) and extract only paragraphs with the relevant information, but initial testsshowed a decline in results due to the great variability of signifiant terms and order of paragraphs.4.1.2. Proximity between applications and job offer using similarity measuresAfter the step of linguistic pre-processing, each document is transformed into a vector with weights characterizing thefrequency of terms Tf. Some tests with Tf-idf (Salton &amp; Mcgill, 1986) were made but they offered no improvement. We haveestablished a strategy using measures of similarity, to rank all applications in relation to a job offer. We combined differentsimilarity measures between the candidate&#8217;s answers (CV and CL) and the associated job offer. We decided to use severalsimilarity measures as defined in Bernstein, Kaufmann, Kiefer, and Bnrki (2005): Cosine (Eq. (1)), which calculates the anglebetween job offer and each candidate answer, Minkowski distances (Eq. (2)) (p = 1 for Manhattan, p = 2 for Euclidean). Thelast measure used is Okabis (Eq. (3)) (Bellot &amp; El-B&#232;ze, 2001). Based on the formula of Okapi (Robertson, Walker, Jones, Han-cock-Beaulieu, &amp; Gatford, 1994), this measure is often used in Information Retrieval. To combine these measures, we use anAlgorithm Decision (AD) (Boudin &amp; Torres Moreno, 2007), which weights the values obtained by each measure of similarity.Several other similarity measures (Overlap, Enertex, Needleman-Wunsch, Jaro-Winkler, Jensen-Shannon divergence) havebeen tested but they are not retained in this study, because the results obtained were disapointing. All measures usedand their combinations are described in Kessler, B&#233;chet, Roche, El-B&#232;ze, and Torres-Moreno (2008a).candidatures rankingInternetJob offerprocessingJob offer publicationCVSplitting candidate&#8217;se-mailsProfilingModule 1Module 2Module 3DescriptionTitleMissionProfileCandidatecompaniesLIACORTEXSystemCLRelevance FeedbackFig. 1. System overview.11 http://sites.univ-provence.fr/veronis/donnees/index.html.12 Lemmatisation finds the root of verbs and transforms plural and/or feminine words into masculine singular form. So we conflate terms developer,development, developing, to develop into develop.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351127Author's personal copycosine&#240;j; d&#222; &#188;Pni&#188;1ji &#65533; diffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiPni&#188;1j2i &#65533; Pni&#188;1d2iq&#240;1&#222;Minkowski&#240;j; d&#222; &#188;11 &#254; Pni&#188;1jji &#65533; dijp&#65533;&#65533;1p&#240;2&#222;Okabis&#240;j; d&#222; &#188; Pi2d\jPni&#188;1ji &#65533; diPni&#188;1ji &#65533; di &#254;ffiffiffiffijdjpMd&#240;3&#222;where j is a job offer, d is a candidate answer, i a term, ji and di occurrence of i respectively in j and d, and Md their averagesize.4.1.3. Relevance FeedbackWe previously changed the system to incorporate a process of Relevance Feedback (Sparck Jones, 1970). Relevance Feed-back is a standard method used particulary for manual query reformulation. For example, the user carefully checks the an-swer set resulting from an initial query, and then reformulates the query. Rocchio&#8217;s algorithm (Rocchio, 1971) and variationshave found wide usage in Information Retrieval and related areas such as Text Categorisation (Joachims, 1997). RelevanceFeedback has been proposed in Smyth and Bradley (2003) to help the user to find a job with server logs from the jobFindersite.13 In our system, Relevance Feedback takes into account the recruiting consultant&#8217;s choice during a first evaluation of a fewCVs. Our goal is not a system capable of finding the best candidate, but a system capable of reproducing the judgement of therecruitment consultant. It is critical for recruiters not to miss a promising candidate that they may have unfortunately rejected.The goal of this Relevance Feedback approach is to help them to avoid this kind of error. We assume that successful candidateshave similar profiles or, at least, that they have much in common. This approach uses documents returned in response to a firstrequest to improve the search results (Salton &amp; Buckley, 1990). In this case, we randomly take a few candidate answers (1&#8211;6 inour experiments) from all relevant candidate answers. These selected candidate answers are added to the job offer. So, we usemanual Relevance Feedback to reflect user judgements in the resulting ranking. We increase the vector representation with theterms from the candidates considered relevant by a recruitment consultant. The system will recompute the similarity betweenthe candidate&#8217;s answer that we evaluate and the job offer enriched with relevant candidates. This allows Sim0 to be recalculedfor each measure of similarity between the application evaluated and the job offer expanded by relevant applications of therelevance feedback process:Sim0measure&#240;j; d&#222; &#188; Simmeasure&#240;j; dkp1k &#65533; &#65533; &#65533; kpn&#222;&#240;4&#222;where j is a job offer, d is a candidate&#8217;s response, pi is a relevant candidate&#8217;s response, n are numbers of retained applicationsfor Relevance Feedback and k is the concatenation operator.The results, presented in Kessler et al. (2009) and hereafter called ISMIS Result showed an improvement in the quality ofthe ranking obtained for each application added to the process of relevance feedback. However, we suspected that a lot ofunnecessary information was still kept in the evaluation and we wanted to use a filter to take into account the content ofsentences. Each document contains additional information (hobbies, greeting and complimentary close, etc.) and standardpre-processing only partially removes it. The idea was to use a system of automatic summarization, coupled to E-Gen, asa powerful filter capable of removing non-essential information contained in CV and Cover Letters.4.2. The CORTEX summarization systemAutomatic summarization is useful to cope with ever increasing volumes of information. An abstract is, by far, the mostconcrete and recognized kind of text condensation. However, the CV is already a kind of summary, with a very importantstructure. We suspect that the filtering system of automatic summarization may not be useful in this case. Since the CL isin free text, we used CORTEX (Torres-Moreno, St-Onge, Gagnon, El-B&#232;ze, &amp; Bellot, 2009, 2001), an efficient state-of-art summa-rization system, in order to retain the more informative segments of the CL.Each document of the application is transmitted to the CORTEX system which provides a summary based on the requestedsize. CORTEX is a document extract summarization system using an optimal decision algorithm that combines several metrics.These metrics result from processing statistical and informational algorithms on the document vector space representation.Fig. 2 presents an overview of the system.The idea is to represent the text in an appropriate vectorial space and apply numeric processings to it. In order to reducecomplexity, a pre-processing of the document is performed: words are filtered, lemmatized, and stemmed. Based on theterms that remain in the text after filtering, a frequency matrix c is built in the following way: Each element cli of this matrixrepresents the number of occurrences of the word i in the sentence l.13 JobFinder (http://www.jobfinder.com).1128R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copyc &#188;c11c12. . .c1i. . .c1NLc21c22. . .c2i. . .c2NL..................cl1cl2. . .cli. . .clNL..................cNS1cNS2. . .cNSi. . .cNSNL2666666666666437777777777775;cli 2 f0; 1; 2; . . .g&#240;5&#222;Another matrix n, called a binary virtual or presence matrix, is defined as:nli &#188;1if cli &#8211; 00elsewhere()&#240;6&#222;Each line of these matrices represents a sentence of the text. Matrices c and cT are the frequency matrix of the sentencesand frequency matrix of the titles respectively.The CORTEX system can use up to C = 11 metrics (Torres-Moreno, Velazquez-Morales, &amp; Meunier, 2002) to evaluate the sen-tence&#8217;s relevance.The system scores each sentence with a decision algorithm which relies on the normalized metrics. Two averages are cal-culated, a positive ks &gt; 0.5, and a negative ks &lt; 0.5 tendency (the case ks = 0.5 is ignored). The following algorithm combinesthe vote of each metric:Ps a &#188; PCv&#188;1kvs&#65533;&#65533;&#65533;&#65533; &#65533; 0:5&#65533;&#65533;;kvs&#65533;&#65533;&#65533;&#65533; &gt; 0:5Psb &#188; PCv&#188;10:5 &#65533; kvs&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;;kvs&#65533;&#65533;&#65533;&#65533; &lt; 0:5Cis the number of metrics and v is the index of the metrics. The value given to each sentence s is calculated with:ifPs a &gt; Psb&#65533;&#65533;then Scorecortexs&#188; 0:5 &#254; Psa=C: retain selse Scorecortexs&#188; 0:5 &#65533; Psb=C: not retain sFig. 2. CORTEX overview.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351129Author's personal copyThe sentences are then ranked according to the obtained values. Depending on the desired compression rate, the sortedsentences will be used to produce the summary. The CORTEX system is applied to each document (Cover Letter) and a sum-mary is generated by concatenating high-scoring sentences. We generated several abstracts with a variable compression rate(5%, 10%, 20%, . . ., 50%, 75% of the size of the documents, in sentences) in order to test the impact of our powerful filter on theE-Gen system. The entire process chain is illustrated in Fig. 1. The best compression rates are generally with 30% (Torres-Moreno et al., 2009). The results are presented in Section 5.3.5. ExperimentsWe selected a data subset from Aktor&#8217;s database composed of 1917 candidates. This subset is called the Mission Corpus. Ithas a size of 10 MB of raw texts and contains 1,375,000 words. The Mission Corpus is composed of a set of 12 job offers cov-ering various themes (jobs in accountancy, business, computer science, etc.) and their candidates. Each Job Offer is associatedwith at least six candidates identified as relevant. As described in Kessler et al. (2008a), each document is segmented to keepthe relevant parts (we remove the description of the company (D) for the job offer). Each candidate answer is tagged as rel-evant or irrelevant. A relevant value corresponds to a potential candidate for a specific job chosen by the recruiting consul-tant. An irrelevant value is associated with an unsuitable candidate for the job (this is a decision made by the manager of ahuman resources company). Our study was conducted on French job offers because the French market represents Aktor&#8217;smain activity. Table 1 shows a few statistics about the Mission Corpus.5.1. Example of CL summariesFig. 3 presents14 an example of an original Cover Letter and Fig. 4. Its corresponding summary15generated by the CORTEX sys-tem with a 30% compression rate (in number of sentences).All the documents of Mission Corpus were previously made anonymous. We observe that the original CL contains anumber of useless information for ranking, such as addresses, phone numbers or form of address at the beginning orend of the letter. The last part of the CL is generally as &#8216;&#8216;Yours faithfully&#8217;&#8217;, &#8216;&#8216;Yours sincerely&#8217;&#8217;, &#8216;&#8216;Best regards&#8217;&#8217;, all of whichrepresent irrelevant information. We further observe in Fig. 4 that the summary obtained with CORTEX removes all thisinformation.5.2. Experimental protocolWe measured the similarity between a job offer and its candidate&#8217;s responses. These measures (Section 4.1.2) rank thecandidate&#8217;s answers by computing a similarity between a job offer and the associated candidate answers. We use theROC curves to evaluate the quality of the ranking obtained. ROC curves (Ferri, Flach, &amp; Hernandez-Orallo, 2002) come fromthe field of signal processing. They are used in medicine to evaluate the validity of diagnostic tests. In our case, ROC curvesshow the rate of irrelevant candidate answers on the X-axis and the rate of relevant candidate answers on the Y-axis. The14 Pierre ASPRE26 years old19 Verdun street 92870 Vannes06-06-06-06-06.Subject: collaboration offerVannes, November 27th, 2008Dear Sir,The Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing profitability. With his legal knowledge in taxand social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices andknowing how to deal with the late settlement of invoices.Therefore I offer my skills. They allow me to:&#8211; Manage with rigueur the accounts of a company.&#8211; Ensure legal compliance activities (payroll, tax billing etc.).&#8211; Provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis.I suggest we meet to discuss all the terms of our future cooperation.I look forward to hearing from you.Best regards.Pierre ASPRE.15 Pierre ASPRESubject: collaboration offerThe Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing profitability. With his legal knowledge in taxand social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices andknowing how to deal with late settlement of invoices.&#8211; ensure legal compliance activities (payroll, tax billing etc.).&#8211; provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis.1130R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copyArea Under the Curve (AUC) can be interpreted as the effectiveness of a measurement of interest. In the case of candidateanswers ranking, a perfect ROC curve corresponds to obtaining all relevant candidate answers at the beginning of the listand all irrelevant ones at the end. This situation corresponds to AUC = 1. The diagonal line corresponds to the performanceof a random system, progress of the rate of relevant candidates being accompanied by an equivalent degradation in the rateof irrelevant candidates. This situation corresponds to AUC = 0.5, as explained in Fawcett (2006). An effective measurementFig. 3. Example of full Cover Letter.Table 1Mission corpus statistics.NumberJob titleNumber of candidate answersNumber ofRelevantIrrelevant34861Sales engineer40142631702Accountant, department suppliers55233233633Sales engineer65184734865Accountant assistant67105734783Accountant assistant108999337463 chefs116605633553Trade commissioner1171710033725Urban sales consultant118437531022Recruitment assistant2212819331274Accountant assistant junior2242619834119Sales assistant2571024731767Accountant assistant junior43751386Total19173231594Fig. 4. Summary of Cover Letter (see Fig. 3) at a 30% compression rate.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351131Author's personal copyof interest to order candidate&#8217;s answers consists in obtaining the highest AUC value. This is strictly equivalent to minimizingthe sum of the ranks of the relevant candidate&#8217;s answers. ROC curves are resistant to imbalance (for example, an imbalancein the number of positive and negative examples) (Roche &amp; Kodratoff, 2006). For each job offer, we evaluated the quality ofthe ranking obtained by this method. Candidate answers considered are only those composed of CV and CL.5.3. ResultsIn this section, we present the results obtained by combining the CORTEX system with the E-Gen ranking application. CORTEXwas used as an additional filter which generates a summary of each document before E-Gen evaluation. We keep the struc-ture of data for job offers as described in Kessler et al. (2008a). A job offer is composed of a Description (D), a Title (T), aMission (M), and a Profile (P). For these experiments, we use two combinations of a job offer content, keeping only Title, Mis-sion, Profile (TMP) and all information of a job offer (DTMP). Results are presented in Tables 2 and 3. Each column presents apart of the application with different sizes of summaries for each line (75%, 50%, . . ., 5%). Full text is a result obtained with100% of the document and was published previously in Kessler et al. (2008a, 2009).Table 2 presents results obtained for each part of the application separately. We observe that AUC of CVs remains belowthe baseline whatever the percentage of compression. We notice however a gradual decrease in AUC scores depending on thepercentage of compression. We explain this by the fact that a CV is already a summary of the most important informationabout the candidates and thereby attempting to summarize degrades final results. We apply the same process with coverletters. Performance is still low overall for CLs in comparison with CVs, however, there is a slight increase in AUC scores witha compression rate of 30%. We explain these results by particular information contained in a cover letter such as the form ofaddress at the beginning or end of the letter (see Fig. 4) which are noise for the ranking system of E-Gen. Results with TMPsegmentation (i.e. conserving only Title, Mission, and Profile of job offer) are of better quality.Table 3 presents the results obtained by combining both parts of the application. Full text values are computed with thewhole documents of the application. The first two columns show the results obtained by combining the summary of the CVand the CL. We observe again a deterioration in the results when trying to summarize the CV. Even if results are lower, itshould be noted, however, that the best score is again obtained at 30%. The last two columns present the results with a sum-marized CL and the full CV. We observe an overall improvement of the AUC score and the best results with a compressionrate of 30% of the Cover Letter.Next step is to combine summaries of the cover letter, which suppresses noise and enriches the offer with the RelevanceFeedback process. Table 4 presents the results obtained with different sizes of Relevance Feedback (RF1 corresponds to oneapplication added to the job offer, RF2 two applications added to the job offer, etc.). Each application added with the rele-vance feedback process consists in a full CV and a summary of the cover letter with a compression rate of 30%. A randomdistribution of applications produces an AUC approximately at 0.5 like explained in Fawcett (2006). We compare ISMIS Resultwith those obtained using a summary of the cover letter. Each test is carried out 100 times with a random distribution ofrelevant applications for Relevance Feedback. Then we compute an average of AUC scores obtained (the curve shows theTable 2Results of CL or CV according to the compression rate of Cortex and part of job offer (with or without Description part).CORTEX compression rate (%)CV + DTMPCV + TMPCL + DTMPCL + TMP100 (full text)0.6220.6480.5670.560750.5650.5750.5630.556500.5580.5690.5530.560400.5520.5650.5610.565300.5490.5600.5690.571200.5200.5580.5640.566100.5590.5590.5430.55450.5500.5420.5210.523Table 3Results for CV and cover letter according to the compression rate.CORTEX compression rate (%)CV and CL summariesFull CV and CL summaryDTMPTMPDTMPTMP100 (full text)0.6340.6420.6340.642750.5210.5810.6390.641500.5560.5510.6430.649400.5440.5680.6430.651300.5700.5870.6460.653200.5690.5330.6410.652100.5640.5340.6310.64550.5460.5470.6380.6491132R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copyaverage for each size). In fact, we compute the Residual Ranking (Billerbeck &amp; Zobel, 2006): Documents that are used for Rel-evance Feedback are removed from the collection before ranking with the reformulated query. We assume that the Rele-vance Feedback process would behave as a reinforcement learning (Sutton &amp; Barto, 1998) but it is impossible toexperiment RFn with n &gt; 6 with this corpus because the number of relevant candidates is too small for some job offers(see Table 1). We observe a slight improvement in results for almost any size of Relevance Feedback. We are conscious thatthe performance gain is low, however, it confirms previous results on the Cover Letter. Fig. 5 shows this improvement. Thisfigure confirms that the addition of just one relevant candidate (RF1) enables the AUC value to be enhanced (i.e. an improve-ment of 0.5&#8211;1.2%). This Relevance Feedback (i.e. RF1) is not very time-consuming for the expert.Fig. 6 shows detailed results of one test. For clarity reasons, we present only 3 of the 12 jobs of our dataset in order tocompare results with and without CORTEX (for each job, RFC are AUC scores with CORTEX and RF without CORTEX).For standard system, we observe a positive progress from 1% to 10% for 10 jobs between RF0 and RF1 (e.g. five jobs havean improvement between 5% and 10%). Note that between RF0 and RF6, 6 jobs have a significant positive progress between10% and 12%. The combination of the E-Gen and CORTEX systems improve standard system results for five jobs from 1% to 5%between RF0 and RF1. Between RF0 and RF6, the Cortex version improves E-Gen&#8217;s results for eight jobs from 1% to 5%.The study of the results shows that job offer 31702 contains some relevant applications with a bad labeling (CV are la-beled CL and CL are only a hyperlink to a CV). The reduction of information on the main document of the application leadsthe system version using summaries to degrade the AUC scores. Job offer 34861 shows a good improvement with each size ofrelevance feedback (RF0:0.65, RF1:0.70, RF6:0.73) and with CORTEX (RF0:0.68, RF1:0.72, RF6:0.79). The detailed study of re-sults shows that job offer 33746 contains some empty applications labeled relevant. This leads the system with and withoutCORTEX to degrade final results. In the same way, an application added without CL explains the identical score in RF2 betweenRF and RFC for job offer 31274.</corp>
  <discussion>No discussion was found</discussion>
  <biblio>ReferencesAudras, I., &amp; Ganascia, J.-G. (2006). Apprentissage du frantais langue TtrangFre et TALN: Analyses de corpus Tcrits a l&#8217;aide d&#8217;outils d&#8217;extraction automatiquedu langage. In J.-M. Viprey (Ed.), 8Fmes JournTes d&#8217;Analyse de DonnTes Textuelles (pp. 67&#8211;78). Univ. de Franche ComtT, Besanton 2006.Bellot, P., &amp; El-B&#232;ze, M. (2001). Classification et segmentation de textes par arbres de dTcision. In TSI (Vol. 20, pp. 107&#8211;134). HermFs.Ben Abdessalem Karaa, W. (2009). Web-based recruiting: A framework for cvs handling. In Second international conference on web and informationtechnologies &#8216;&#8216;ICWIT&#8217;09&#8217;&#8217;, kerkennah Island, Sfax, Tunisia, June 12&#8211;14 (pp. 395&#8211;406).Bernstein, A., Kaufmann, E., Kiefer, C., &amp; Bnrki, C. (2005). Simpack: A generic java library for similarity measures in ontologies. Tech. rep., University of ZurichDepartment of Informatics.Billerbeck, B., &amp; Zobel, J. (2006). Efficient query expansion with auxiliary data structures. Information Systems, 31(7), 573&#8211;584.Boudin, F., &amp; Torres Moreno, J. M. (2007). Neo-cortex: A performant user-oriented multi-document summarization system. In CICLing (pp. 551&#8211;562).Bourse, M., LeclFre, M., Morin, E., &amp; Trichet, F. (2004). Human resource management and semantic web technologies. In ICTTA 2004 Damascus Syria (pp. 641&#8211;642).Cazalens, S., &amp; Lamarre, P. (2001). An organization of internet agents based on a hierarchy of information domains. In Proceedings MAAMAW&#8217;2001, Annecy,France (pp. 573&#8211;584).Clech, J., &amp; Zighed, D. A. (2003). Data mining et analyse des cv: une exp&#233;rience et des perspectives. In EGC&#8217;03 Revue des Sciences et Technologies del&#8217;Information (Vol. 17, pp. 83&#8211;92). Lyon.Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F. M., Mongiello, M., &amp; Mottola, M. (2003). A formal approach to ontology-based semantic match of skillsdescriptions. Journal of Universal Computer Science, Special issue on Skills Management, 9, 1437&#8211;1454.Dorn, J., &amp; Naz, T. (2007). Meta-search in human resource management. In Proceedings of 4th international conference on knowledge systems ICKS&#8217;07Bangkok,Thailand (pp. 105&#8211;110).Enrica, A., &amp; Iezzi, D. F. (2006). Recruitment via web and information technology: A model for ranking the competences in job market. In JADT&#8217;2006,Besanton, France (pp. 79&#8211;88).Fan, R.-E., Chen, P.-H., &amp; Lin, C.-J. (2005). Working set selection using the second order information for training SVM. Journal of Machine Learning Research,1889&#8211;1918.Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27, 861&#8211;874.Ferri, C., Flach, P., &amp; Hernandez-Orallo, J. (2002). Learning decision trees using the area under the ROC curve. In Proceedings of ICML 2002: Sydney, NSW,Australia (pp. 139&#8211;146).Gorenak, I., &amp; Mlaker KaF, S. S. O. (2010). Cross-cultural comparison of online job advertisements. JLST, Journal of Logistics and Sustainable Transport, 2, 37&#8211;52.RF0RF1RF2RF3RF4RF5RF60,400,450,500,550,600,650,700,750,800,85AUC scoreRelevance Feeback size 34861 RF 34861 RFC 31274 RF 31274 RFC 31702 RF 31702 RFCFig. 6. Comparison of detailed results for 3 jobs with and without summaries of CL. For each job, RFC means AUC scores with CORTEX and RF without CORTEX.1134R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;1135Author's personal copyJoachims, T. (1997). A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. In ICML 1997, Nashville, Tennessee, USA (pp. 143&#8211;151).San Francisco, CA, USA.Kessler, R., B&#233;chet, N., Roche, M., El-B&#232;ze, M., &amp; Torres-Moreno, J. M. (2008a). Automatic profiling system for ranking candidates answers in humanresources. In OTM &#8217;08 in Monterrey, Mexico (pp. 625&#8211;634).Kessler, R., B&#233;chet, N., Roche, M., El-B&#232;ze, M., &amp; Torres-Moreno, J. M. (2009). Job offer management: How improve the ranking of candidates. Prague: ISMIS.431&#8211;441.Kessler, R., Torres-Moreno, J. M., &amp; El-B&#232;ze, M. (2007). E-Gen: Automatic job offer processing system for human ressources. In MICAI, Aguscalientes, Mexique(pp. 985&#8211;995).Kessler, R., Torres-Moreno, J. M., &amp; El-B&#232;ze, M. (2008b). E-Gen: Profilage automatique de candidatures. In TALN 2008, Avignon, France (pp. 370&#8211;379).Loth, R., Battistelli, D., Chaumartin, F., De Mazancourt, H., Minel, J. L., &amp; Vinckx, A. (2010). Linguistic information extraction for job ads (SIRE project). InRIAO&#8217;2010 9th conference 28&#8211;30 April, Paris, France (pp. 300&#8211;303).Marchal, E., Mellet, K., &amp; Rieucau, G. (2007). Job board toolkits: Internet matchmaking and changes in job advertisements. Human Relations, 60(7),1091&#8211;1113.Mocho, M., Paslaru, E., &amp; Simperl, B. (2006). Practical guidelines for building semantic e-recruitment applications. In I-Know&#8217;06 special track on advancedsemantic technologies, Graz, Austria, September 2006.Morin, E., LeclFre, M., &amp; Trichet, F. (2004). The semantic web in e-recruitment. In The first European symposium of semantic Web (ESWS&#8217;2004) (pp. 67&#8211;78).Quilan, J. (1993). C4.5: Programs for machine learning. San Mateo, CA, San Francisco, CA, USA: Morgan Kaufmann.Rafter, R., Bradley, K., &amp; Smyt, B. (2000). Automated collaborative filtering applications for online recruitment services. In International conference on adaptivehypermedia and adaptive web-based systems, Trento, Italy (pp. 363&#8211;368).Robertson, S., Walker, S., Jones, S., Hancock-Beaulieu, M. M., &amp; Gatford, M. (1994). Okapi at trec-3. NIST Special Publication 500-225: TREC-3, pp. 109&#8211;126.Rocchio, J. (1971). Relevance feedback in information retrieval. In The smart system: Experiments in automatic document processing (pp. 313&#8211;323). Prentice-Hall.Roche, M., &amp; Kodratoff, Y., 2006. Pruning terminology extracted from a specialized corpus for CV ontology acquisition. In OTM&#8217;06, Montpellier, France (pp.1107&#8211;1116).Roche, M., &amp; Prince, V. (2008). Evaluation et dTtermination de la pertinence pour des syntagmes candidats a la collocation. In JADT (pp. 1009&#8211;1020).Salton, G., &amp; Buckley, C. (1990). Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 288&#8211;297.Salton, G., &amp; Mcgill, M. J. (1986). Introduction to modern information retrieval. New York, NY, USA: McGraw-Hill Inc.Smyth, B., &amp; Bradley, K. (2003). Personalized information ordering: A case-study in online recruitment. Journal of Knowledge-Based Systems, 269&#8211;275.Sparck Jones, K. (1970). Some thoughts on classification for retrieval. Journal of Documentation, 89&#8211;101.Sutton, R. S., &amp; Barto, A. G. (1998). Reinforcement learning: An introduction (adaptive computation and machine learning). The MIT Press.Tolksdorf,R.,Mocho,M.,Heese,R.,Oldakowski,R.,&amp;Christian,B.(2006).Semantic-Web-TechnologienimArbeitsvermittlungsprozess.Wirtschaftsinformatik, 17&#8211;26.Torres-Moreno, J. M., Vel&#225;zquez-Morales, P., &amp; Meunier, M. (2001). CORTEX, un algorithme pour la condensation automatique de textes. In ARCo (Vol. 2, pp.365&#8211;371).Torres-Moreno, J. M., St-Onge, P.-L., Gagnon, M., El-B&#232;ze, M., &amp; Bellot, P. (2009). Automatic summarization system coupled with a question-answeringsystem (qaas). In CoRR abs/0905.2990.Torres-Moreno, J. M., Velazquez-Morales, P., &amp; Meunier, J. (2002). Condens&#233;s de textes par des m&#233;thodes num&#233;riques. JADT, St Malo, France, 2, 723&#8211;734.Viterbi, A. J. (1967). Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13,260&#8211;269.Yahiaoui, L., Boufa&#305;&#168;da, Z., &amp; Pri&#233;, Y. (2006). Semantic annotation of documents applied to e-recruitment. In SWAP 2006 &#8211; Semantic web applications andperspectives. ISSN: 1613-0073.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351135</biblio>
  <conclusion>6. Conclusion and future workJob offer processing is a difficult and highly subjective task. The retrieval of relevant information concerning job descrip-tions and skills is not a trivial task (Loth et al., 2010) and results on this type of document have been quite low (Clech &amp;Table 4Comparison of AUC score for each size of Relevance Feedback with CORTEX summarization system.Size of Relevance FeedbackISMIS resultFull CV and CL summary 30% compression rateRandom distribution0.5000.500RF00.6420.653RF10.6540.658RF20.6570.659RF30.6590.661RF40.6590.659RF50.6600.662RF60.6610.663Fig. 5. Results of Relevance Feedback with and without summaries of CL.R. Kessler et al. / Information Processing and Management 48 (2012) 1124&#8211;11351133Author's personal copyZighed, 2003). The information we use in this kind of process is not well formated in natural language, but follows a conven-tional structure. This paper deals with the CORTEX summarizer and the E-Gen system for processing job offers. E-Gen assists anemployer in the recruitment task. This paper focuses on candidate answers to job offers. We rank the candidate answers byusing different similarity measures and different document representations in a vector space model. We use a process of rel-evance feedback to perform reinforcement learning, whereby each new application added to the process assists in the deci-sion-making. We choose to evaluate the quality of our approaches by computing Area Under the Curve.CORTEX is asummarization system using an optimal decision algorithm that combines several metrics. We present the results obtainedby combining both systems. AUC obtained with summarized cover letter at 30% of compression size and a full CV shows aslight improvement in the results. As future work, we plan to apply other techniques, such as finding discriminant features ofirrelevant applications using the Rocchio algorithm (Rocchio, 1971), weighting the different parts of an application, etc. inorder to improve results. We also plan to use a categorization of jobs to take into consideration similar jobs, such as &#8217;&#8217;devel-oper&#8217;&#8217; and &#8216;&#8216;programmer&#8217;&#8217;. Finally we propose to measure the CV quality by building an evaluation on an Internet portal. Ouraim with this evaluation is to present a job-seeker with a list of the most suitable job ads according to his profile.</conclusion>
</article>
