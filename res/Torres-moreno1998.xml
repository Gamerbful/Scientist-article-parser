<article>
  <preamble>Torres-moreno1998.pdf</preamble>
  <titre>LETTER </titre>
  <auteurs>
    <auteur>Scott Fahlman </auteur>
    <affiliation>No affiliation</affiliation>
    <auteur>J. Manuel Torres Moreno </auteur>
    <affiliation>No affiliation</affiliation>
    <auteur>Mirta B. Gordon</auteur>
    <affiliation>No affiliation</affiliation>
  </auteurs>
  <abstract>No abstract was found</abstract>
  <biblio>ReferencesAlpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Un-published doctoral dissertation, Ecole Polytechnique Federale de Lausanne,Switzerland.Biehl, M., &amp; Opper, M. (1991). Tilinglike learning in the parity machine. PhysicalReview A, 44, 6888.Bottou, L., &amp; Vapnik, V. (1992). Local learning algorithms. Neural Computation,4(6), 888&#8211;900.Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Departmentof Statistics, University of California at Berkeley.Breiman, L., Friedman, J. H., Olshen, R. A., &amp; Stone, C. J. (1984). Classificationand regression trees. Monterey, CA: Wadsworth and Brooks/Cole.Denker,J.,Schwartz,D.,Wittner,B.,Solla,S.,Howard,R.,Jackel,L.,&amp;Hopfield,J.(1987). Large automatic learning, rule extraction, and generalization. ComplexSystems, 1, 877&#8211;922.Depenau, J. (1995). Automated design of neural network architecture for classification.Unpublished doctoral dissertation, Computer Science Department, AarhusUniversity.Drucker, H., Schapire, R., &amp; Simard, P. (1993). Improving performance in neu-ral networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &amp;C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42&#8211;49). San Mateo, CA: Morgan Kaufmann.Fahlman, S. E., &amp; Lebiere, C. (1990). The cascade-correlation learning architec-ture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,2 (pp. 524&#8211;532). San Mateo: Morgan Kaufmann.Farrell, K. R., &amp; Mammone, R. J. (1994). Speaker recognition using neural treenetworks. In J. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in NeuralInformation Processing Systems, 6 (pp. 1035&#8211;1042). San Mateo, CA: MorganKaufmann.Frean, M. (1990). The Upstart algorithm: A method for constructing and trainingfeedforward neural networks. Neural Computation, 2(2), 198&#8211;209.Frean, M. (1992). A &#8220;thermal&#8221; perceptron learning rule. Neural Computation, 4(6),946&#8211;957.Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.Fritzke, B. (1994). Supervised learning with growing cell structures. InJ. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in neural informa-tion processing systems, 6 (pp. 255&#8211;262). San Mateo, CA: Morgan Kaufmann.Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. PatternRecognition, Oct. 28&#8211;31, Paris, vol. 4.Classification Tasks with Binary Units1029Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).5emes Journees Nationales du PRC-IA Teknea, Nancy.Geman, S., Bienenstock, E., &amp; Doursat, R. (1992). Neural networks and thebias/variance dilemma. Neural Computation, 4(1), 1&#8211;58.Goodman, R. M., Smyth, P., Higgins, C. M., &amp; Miller, J. W. (1992). Rule-basedneural networks for classification and probability estimation. Neural Compu-tation, 4(6), 781&#8211;804.Gordon, M. B. (1996). A convergence theorem for incremental learning with real-valued inputs. In IEEE International Conference on Neural Networks, pp. 381&#8211;386.Gordon, M. B., &amp; Berchier, D. (1993). Minimerror: A perceptron learning rulethat finds the optimal weights. In M. Verleysen (Ed.), European Symposium onArtificial Neural Networks (pp. 105&#8211;110). Brussels: D Facto.Gordon, M. B., &amp; Grempel, D. (1995). Optimal learning with a temperaturedependent algorithm. Europhysics Letters, 29(3), 257&#8211;262.Gordon, M. B., Peretto, P., &amp; Berchier, D. (1993). Learning algorithms for percep-trons from statistical physics. Journal of Physics I (France), 3, 377&#8211;387.Gorman, R. P., &amp; Sejnowski, T. J. (1988). Analysis of hidden units in a layerednetwork trained to classify sonar targets. Neural Networks, 1, 75&#8211;89.Gyorgyi, G., &amp; Tishby, N. (1990). Statistical theory of learning a rule. InW. K. Theumann &amp; R. Koeberle (Eds.), Neural networks and spin glasses. Sin-gapore: World Scientific.Hoehfeld, M., &amp; Fahlman, S. (1991). Learning with limited numerical precision usingthe cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:Carnegie Mellon University.Knerr, S., Personnaz, L., &amp; Dreyfus, G. (1990). Single-layer learning revisited: Astepwise procedure for building and training a neural network. In J. Herault&amp; F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications(pp. 41&#8211;50). Berlin: Springer-Verlag.Marchand, M., Golea, M., &amp; Ruj&#180;an, P. (1990). A convergence theorem for sequen-tial learning in two-layer perceptrons. Europhysics Letters, 11, 487&#8211;492.Martinez, D., &amp; Esteve, D. (1992). The offset algorithm: Building and learningmethod for multilayer neural networks. Europhysics Letters, 18, 95&#8211;100.Mezard, M., &amp; Nadal, J.-P. (1989). Learning in feedforward layered networks:The Tiling algorithm. J. Phys. A: Math. and Gen., 22, 2191&#8211;2203.Mukhopadhyay, S., Roy, A., Kim, L. S., &amp; Govil, S. (1993). A polynomial time al-gorithm for generating neural networks for pattern classification: Its stabilityproperties and some test results. Neural Computation, 5(2), 317&#8211;330.Nadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural net-work. Int. J. Neur. Syst., 1, 55&#8211;59.Prechelt, L. (1994). PROBEN1&#8212;A set of benchmarks and benchmarking rules for neu-ral network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,Faculty of Informatics.Raffin, B., &amp; Gordon, M. B. (1995). Learning and generalization with Minimerror,a temperature dependent learning algorithm. Neural Computation, 7(6), 1206&#8211;1224.1030J. Manuel Torres Moreno and Mirta B. GordonReilly, D. E, Cooper, L. N., &amp; Elbaum, C. (1982). A neural model for categorylearning. Biological Cybernetics, 45, 35&#8211;41.Roy, A., Kim, L., &amp; Mukhopadhyay, S. (1993). A polynomial time algorithmfor the construction and training of a class of multilayer perceptron. NeuralNetworks, 6(1), 535&#8211;545.Sirat, J. A., &amp; Nadal, J.-P. (1990). Neural trees: A new tool for classification.Network, 1, 423&#8211;438.Solla, S. A. (1989). Learning and generalization in layered neural networks: Thecontiguity problem. In L. Personnaz &amp; G. Dreyfus (Eds.), Neural Networksfrom Models to Applications. Paris: I.D.S.E.T.Torres Moreno, J.-M., &amp; Gordon, M. B. (1995). An evolutive architecture coupledwith optimal perceptron learning for classification. In M. Verleysen (Ed.),European Symposium on Artificial Neural Networks. Brussels: D Facto.Torres Moreno, J.-M., &amp; Gordon, M. B. (1998). Characterization of the sonarsignals benchmark. Neural Proc. Letters, 7(1), 1&#8211;4.Trhun, S. B., et al. (1991). The monk&#8217;s problems: A performance comparison of differentlearning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: CarnegieMellon University.Vapnik, V. (1992). Principles of risk minimization for learning theory. InJ. E. Moody, S. J. Hanson, &amp; R. P. Lippmann (Eds.), Advances in neural informa-tion processing systems, 4 (pp. 831&#8211;838). San Mateo, CA: Morgan Kaufmann.Verma, B. K., &amp; Mulawka, J. J. (1995). A new algorithm for feedforward neu-ral networks. In M. Verleysen (Ed.), European Symposium on Artificial NeuralNetworks (pp. 359&#8211;364). Brussels: D Facto.Wolberg, W. H., &amp; Mangasarian, O. L. (1990). Multisurface method of patternseparation for medical diagnosis applied to breast cytology. In Proceedings ofthe National Academy of Sciences, USA, 87, 9193&#8211;9196.Received February 13, 1997; accepted September 4, 1997.This article has been cited by:1. C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectralanalysis. IEEE Transactions on Neural Networks 10, 725-740. [CrossRef]2. Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward NeuralNetworks. International Journal of Neural Systems 08, 647-659. [CrossRef]</biblio>
  <conclusion>6 ConclusionWe presented an incremental learning algorithm for classification, which wecall NetLines. It generates small feedforward neural networks with a singlehidden layer of binary units connected to a binary output neuron. NetLinesallows for an automatic adaptation of the neural network to the complexityof the particular task. This is achieved by coupling an error-correcting strat-egy for the successive addition of hidden neurons with Minimerror, a very1026J. Manuel Torres Moreno and Mirta B. Gordon&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;1HW/LQHV &#65533;YRWH&#65533;1HW/LQHV &#65533;:7$&#65533;,5,6 GDWDEDVH&#949;J1XPEHU RI ZHLJKWVFigure 7: Iris database: Generalization error &#1013;g versus number of parameters.1: offset, 2: backpropagation (Martinez &amp; Esteve, 1992); 4,5: backpropagation(Verma &amp; Mulawka, 1995); 3,6: gradient-descent orthogonalized training (Verma&amp; Mulawka, 1995).efficient perceptron training algorithm. Learning is fast not only becauseit reduces the problem to that of training single perceptrons, but mainlybecause there is no longer a need for the usual preliminary tests required todetermine the correct architecture for the particular application. Theoremsvalid for binary as well as for real-valued inputs guarantee the existence ofa solution with a bounded number of hidden neurons obeying the growthstrategy.The networks are composed of binary hidden units whose states consti-tute a faithful encoding of the input patterns. They implement a mappingfrom the input space to a discrete H-dimensional hidden space, H beingthe number of hidden neurons. Thus, each pattern is labeled with a binaryword of H bits. This encoding may be seen as a compression of the pattern&#8217;sinformation. The hidden neurons define linear boundaries, or portions ofboundaries, between classes in input space. The network&#8217;s output may begiven a probabilistic interpretation based on the distance of the patterns tothese boundaries.Tests on several benchmarks showed that the networks generated by ourincremental strategy are small, in spite of the fact that the hidden neuronsare appended until error-free learning is reached. Even when the networksobtained with NetLines are larger than those used by other algorithms, itsgeneralization error remains among the smallest values reported. In noisyor difficult problems, it may be useful to stop the network&#8217;s growth beforeClassification Tasks with Binary Units1027the condition of zero training errors is reached. This decreases overfitting, assmaller networks (with less parameters) are thus generated. However, theprediction quality (measured by the generalization error) of the classifiersgenerated with NetLines is not improved by early stopping.Our results were obtained without cross-validation or any data manip-ulation like boosting, bagging, or arcing (Breiman, 1994; Drucker, Schapire,&amp; Simard, 1993). Those costly procedures combine results of very largenumbers of classifiers, with the aim of improving the generalization perfor-mance through the reduction of the variance. Because NetLines is a stableclassifier, presenting small variance, we do not expect that such techniqueswould significantly improve our results.AppendixIn this appendix we exhibit a particular solution to the learning strategy ofNetLines. This solution is built in such a way that the cardinal of a convexsubset of well-learned patterns, Lh, grows monotonically upon the additionof hidden units. Because this cardinal cannot be larger than the total numberof training patterns, the algorithm must stop with a finite number of hiddenunits.Suppose that h hidden units have already been included and that theoutput neuron still makes classification errors on patterns of the training set,called training errors. Among these wrongly learned patterns, let &#957; be theone closest to the hyperplane normal to &#8407;wh, called hyperplane-h hereafter.We define Lh as the subset of (correctly learned) patterns lying closer tohyperplane-h than &#8407;&#958;&#957;. Patterns in Lh have 0 &lt; &#947;h &lt; |&#947; &#957;h |. The subset Lh andat least pattern &#957; are well learned if the next hidden unit, h+1, has weights:&#8407;wh+1 = &#964; &#957;h &#8407;wh &#8722; (1 &#8722; &#1013;h)&#964; &#957;h ( &#8407;wh &#183; &#8407;&#958;&#957;)&#710;e0,(A.1)where &#710;e0 &#8801; (1, 0, . . . , 0). The conditions that both Lh and pattern &#957; havepositive stabilities (are correctly learned) impose that0 &lt; &#1013;h &lt; min&#181;&#8712;Lh|&#947; &#957;h | &#8722; &#947; &#181;h|&#947; &#957;h |.(A.2)The following weights between the hidden units and the output will givethe correct output to pattern &#957; and to the patterns of Lh:W0(h + 1) = W0(h) + &#964; &#957;(A.3)Wi(h + 1) = Wi(h) for 1 &#8804; i &#8804; h(A.4)Wh+1(h + 1) = &#8722;&#964; &#957;.(A.5)Thus, card(Lh+1) &#8805; card(Lh) + 1. As the number of patterns in Lh increasesmonotonically with h, convergence is guaranteed with less than P hiddenunits.1028J. Manuel Torres Moreno and Mirta B. GordonAcknowledgmentsJ.M. thanks Consejo Nacional de Ciencia y Tecnolog&#180;&#305;a and UniversidadAut&#180;onoma Metropolitana, Mexico, for financial support (grant 65659).</conclusion>
</article>
