<article>
  <preamble>Torres.pdf</preamble>
  <titre>Summary Evaluation with and without References </titre>
  <auteur>Juan-Manuel Torres-Moreno , Horacio Saggion , Eric SanJuan , Patricia Vel&#180;azquez-Morales</auteur>
  <abstract>Abstract&#8212;Westudyanewcontent-basedmethodfortheevaluationoftextsummarizationsystemswithouthuman models which is used to produce system rankings.Theresearchiscarriedoutusinganewcontent-basedevaluation framework called FRESA to compute a variety ofdivergences among probability distributions. We apply ourcomparison framework to various well-established content-basedevaluation measures in text summarization such as COVERAGE,RESPONSIVENESS,PYRAMIDSandROUGEstudyingtheirassociations in various text summarization tasks includinggeneric multi-document summarization in English and French,focus-basedmulti-documentsummarizationinEnglishandgeneric single-document summarization in French and Spanish.Index Terms&#8212;Text summarization evaluation, content-basedevaluation measures, divergences.</abstract>
  <discussion>The departing point for our inquiry into text summarizationevaluation has been recent work on the use of content-basedevaluation metrics that do not rely on human models but thatcompare summary content to input content directly [12]. Wehave some positive and some negative results regarding thedirect use of the full document in content-based evaluation.We have verified that in both generic muti-documentsummarizationandintopic-basedmulti-documentsummarizationinEnglishcorrelationamongmeasuresthat use human models (PYRAMIDS,RESPONSIVENESSand ROUGE) and a measure that does not use models(J S divergence) is strong. We have found that correlationamong the same measures is weak for summarization ofbiographical information and summarization of opinions inblogs. We believe that in these cases content-based measuresshould be considered, in addition to the input document, thesummarization task (i.e. text-based representation, description)to better assess the content of the peers [25], the task being adeterminant factor in the selection of content for the summary.Our multi-lingual experiments in generic single-documentsummarizationconfirmastrongcorrelationamongtheJ S divergence and ROUGE measures. It is worth notingthatROUGEisingeneralthechosenframeworkforpresenting content-based evaluation results in non-Englishsummarization.For the experiments in Spanish, we are conscious that weonly have one model summary to compare with the peers.Nevertheless, these models are the corresponding abstractswritten by the authors. As the experiments in [26] show, theprofessionals of a specialized domain (as, for example, themedical domain) adopt similar strategies to summarize theirtexts and they tend to choose roughly the same content chunksfor their summaries. Previous studies have shown that authorabstracts are able to reformulate content with fidelity [27] andthese abstracts are ideal candidates for comparison purposes.Because of this, the summary of the author of a medical articlecan be taken as reference for summaries evaluation. It is worthnoting that there is still debate on the number of models to beused in summarization evaluation [28]. In the French corpusPISTES, we suspect the situation is similar to the Spanishcase.</discussion>
  <biblio>No references was found</biblio>
</article>
